{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45df4411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI model for summarization...\n",
      "Model loaded successfully!\n",
      "\n",
      "============================================================\n",
      "Scraping content from: https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html#google_vignette\n",
      "Scraped 28745 characters\n",
      "Generating summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 191. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 200, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 200, but your input_length is only 145. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html#google_vignette\n",
      "Original length: 28745 characters\n",
      "Summary length: 1156 characters\n",
      "Compression: 4.0%\n",
      "\n",
      "Summary:\n",
      "Wellness startup Biopeak raises $3.5 million in seed funding from Ranjan Pai office, Accel's Prashanth Prakash, others. The company, which opened its first clinic in Bengaluru in March 2025, combines molecular diagnostics, high-resolution imaging, and non-invasive testing. Biopeak provides personalised health services through in-house clinics. Its approach combines molecular diagnostics, high-resolution imaging, and non-invasive testing. At the core of Biopeak’s offering is a diagnostic system that includes tests like organic acid profiling, microbiome mapping, salivary cortisol rhythms, and whole-exome functional genomics. The company opened its first clinic in Bengaluru in March 2025. Each client typically undergoes over six hours of multidisciplinary consultations, guided by a dedicated health manager and a team of specialists. Biopeak primarily serves two categories of users: those seeking to improve overall health and performance, and individuals managing chronic or complex conditions that may not be adequately addressed by conventional treatment. The resulting health plan is revisited over time and refined through follow-up testing.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class WebScraperSummarizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the web scraper with AI summarization capabilities\"\"\"\n",
    "        print(\"Loading AI model for summarization...\")\n",
    "        # Using facebook/bart-large-cnn - free, open-source, good for summarization\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\", \n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            device=-1  # Use CPU (set to 0 for GPU if available)\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def scrape_content(self, url):\n",
    "        \"\"\"Scrape text content from the given URL\"\"\"\n",
    "        try:\n",
    "            # Set headers to mimic a real browser\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            # Make request\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Extract text from common content containers\n",
    "            text_content = \"\"\n",
    "            \n",
    "            # Try to find main content areas\n",
    "            content_selectors = [\n",
    "                'article', 'main', '.content', '#content', \n",
    "                '.post', '.entry', 'div.text', 'div.body'\n",
    "            ]\n",
    "            \n",
    "            for selector in content_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    text_content = \" \".join([elem.get_text() for elem in elements])\n",
    "                    break\n",
    "            \n",
    "            # If no specific content area found, get all paragraph text\n",
    "            if not text_content:\n",
    "                paragraphs = soup.find_all(['p', 'div', 'span'])\n",
    "                text_content = \" \".join([p.get_text() for p in paragraphs])\n",
    "            \n",
    "            # Clean up the text\n",
    "            text_content = re.sub(r'\\s+', ' ', text_content)  # Replace multiple spaces\n",
    "            text_content = text_content.strip()\n",
    "            \n",
    "            return text_content\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error fetching URL: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing content: {str(e)}\"\n",
    "    \n",
    "    def chunk_text(self, text, max_chunk_size=1000):\n",
    "        \"\"\"Split text into chunks for processing\"\"\"\n",
    "        sentences = text.split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk + sentence) < max_chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def summarize_text(self, text, max_length=150, min_length=50):\n",
    "        \"\"\"Summarize text using AI model\"\"\"\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return \"Text too short to summarize meaningfully.\"\n",
    "        \n",
    "        try:\n",
    "            # If text is very long, chunk it and summarize each chunk\n",
    "            if len(text) > 1000:\n",
    "                chunks = self.chunk_text(text)\n",
    "                chunk_summaries = []\n",
    "                \n",
    "                for chunk in chunks[:5]:  # Limit to first 5 chunks to avoid overload\n",
    "                    summary = self.summarizer(\n",
    "                        chunk, \n",
    "                        max_length=max_length//len(chunks[:5]), \n",
    "                        min_length=min_length//len(chunks[:5]), \n",
    "                        do_sample=False\n",
    "                    )\n",
    "                    chunk_summaries.append(summary[0]['summary_text'])\n",
    "                \n",
    "                # Combine and re-summarize if needed\n",
    "                combined_summary = \" \".join(chunk_summaries)\n",
    "                if len(combined_summary) > max_length * 2:\n",
    "                    final_summary = self.summarizer(\n",
    "                        combined_summary,\n",
    "                        max_length=max_length,\n",
    "                        min_length=min_length,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "                    return final_summary[0]['summary_text']\n",
    "                else:\n",
    "                    return combined_summary\n",
    "            else:\n",
    "                # Direct summarization for shorter text\n",
    "                summary = self.summarizer(\n",
    "                    text,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                return summary[0]['summary_text']\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error during summarization: {str(e)}\"\n",
    "    \n",
    "    def process_url(self, url, summary_length=\"medium\"):\n",
    "        \"\"\"Main method to scrape URL and return summary\"\"\"\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "        \n",
    "        # Scrape content\n",
    "        content = self.scrape_content(url)\n",
    "        \n",
    "        if content.startswith(\"Error\"):\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"error\": content,\n",
    "                \"summary\": None,\n",
    "                \"original_length\": 0\n",
    "            }\n",
    "        \n",
    "        print(f\"Scraped {len(content)} characters\")\n",
    "        \n",
    "        # Set summary parameters based on desired length\n",
    "        length_params = {\n",
    "            \"short\": {\"max_length\": 1000, \"min_length\": 30},\n",
    "            \"medium\": {\"max_length\": 1000, \"min_length\": 50},\n",
    "            \"long\": {\"max_length\": 1000, \"min_length\": 100}\n",
    "        }\n",
    "        \n",
    "        params = length_params.get(summary_length, length_params[\"long\"])\n",
    "        \n",
    "        print(\"Generating summary...\")\n",
    "        summary = self.summarize_text(content, **params)\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"summary\": summary,\n",
    "            \"original_length\": len(content),\n",
    "            \"summary_length\": len(summary),\n",
    "            \"compression_ratio\": f\"{len(summary)/len(content)*100:.1f}%\"\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "    scraper = WebScraperSummarizer()\n",
    "    \n",
    "    # Example URLs - replace with your target URLs\n",
    "    test_urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html#google_vignette\"\n",
    "    ]\n",
    "    \n",
    "    for url in test_urls:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        result = scraper.process_url(url, summary_length=\"medium\")\n",
    "        \n",
    "        if result.get(\"error\"):\n",
    "            print(f\"Failed to process {url}\")\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"URL: {result['url']}\")\n",
    "            print(f\"Original length: {result['original_length']} characters\")\n",
    "            print(f\"Summary length: {result['summary_length']} characters\")\n",
    "            print(f\"Compression: {result['compression_ratio']}\")\n",
    "            print(f\"\\nSummary:\\n{result['summary']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages first:\n",
    "    # pip install requests beautifulsoup4 transformers torch\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e511cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI model for summarization...\n",
      "Model loaded successfully!\n",
      "\n",
      "============================================================\n",
      "Scraping content from: https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html#google_vignette\n",
      "Scraped 28745 characters\n",
      "Generating summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 191. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 200, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 200, but your input_length is only 145. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html#google_vignette\n",
      "Original length: 28745 characters\n",
      "Summary length: 1156 characters\n",
      "Compression: 4.0%\n",
      "\n",
      "Summary:\n",
      "Wellness startup Biopeak raises $3.5 million in seed funding from Ranjan Pai office, Accel's Prashanth Prakash, others. The company, which opened its first clinic in Bengaluru in March 2025, combines molecular diagnostics, high-resolution imaging, and non-invasive testing. Biopeak provides personalised health services through in-house clinics. Its approach combines molecular diagnostics, high-resolution imaging, and non-invasive testing. At the core of Biopeak’s offering is a diagnostic system that includes tests like organic acid profiling, microbiome mapping, salivary cortisol rhythms, and whole-exome functional genomics. The company opened its first clinic in Bengaluru in March 2025. Each client typically undergoes over six hours of multidisciplinary consultations, guided by a dedicated health manager and a team of specialists. Biopeak primarily serves two categories of users: those seeking to improve overall health and performance, and individuals managing chronic or complex conditions that may not be adequately addressed by conventional treatment. The resulting health plan is revisited over time and refined through follow-up testing.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class WebScraperSummarizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the web scraper with AI summarization capabilities\"\"\"\n",
    "        print(\"Loading AI model for summarization...\")\n",
    "        # Using facebook/bart-large-cnn - free, open-source, good for summarization\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\", \n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            device=-1  # Use CPU (set to 0 for GPU if available)\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def scrape_content(self, url):\n",
    "        \"\"\"Scrape text content from the given URL\"\"\"\n",
    "        try:\n",
    "            # Set headers to mimic a real browser\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            # Make request\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Extract text from common content containers\n",
    "            text_content = \"\"\n",
    "            \n",
    "            # Try to find main content areas\n",
    "            content_selectors = [\n",
    "                'article', 'main', '.content', '#content', \n",
    "                '.post', '.entry', 'div.text', 'div.body'\n",
    "            ]\n",
    "            \n",
    "            for selector in content_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    text_content = \" \".join([elem.get_text() for elem in elements])\n",
    "                    break\n",
    "            \n",
    "            # If no specific content area found, get all paragraph text\n",
    "            if not text_content:\n",
    "                paragraphs = soup.find_all(['p', 'div', 'span'])\n",
    "                text_content = \" \".join([p.get_text() for p in paragraphs])\n",
    "            \n",
    "            # Clean up the text\n",
    "            text_content = re.sub(r'\\s+', ' ', text_content)  # Replace multiple spaces\n",
    "            text_content = text_content.strip()\n",
    "            \n",
    "            return text_content\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error fetching URL: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing content: {str(e)}\"\n",
    "    \n",
    "    def chunk_text(self, text, max_chunk_size=1000):\n",
    "        \"\"\"Split text into chunks for processing\"\"\"\n",
    "        sentences = text.split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk + sentence) < max_chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def summarize_text(self, text, max_length=150, min_length=50):\n",
    "        \"\"\"Summarize text using AI model\"\"\"\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return \"Text too short to summarize meaningfully.\"\n",
    "        \n",
    "        try:\n",
    "            # If text is very long, chunk it and summarize each chunk\n",
    "            if len(text) > 1000:\n",
    "                chunks = self.chunk_text(text)\n",
    "                chunk_summaries = []\n",
    "                \n",
    "                for chunk in chunks[:5]:  # Limit to first 5 chunks to avoid overload\n",
    "                    summary = self.summarizer(\n",
    "                        chunk, \n",
    "                        max_length=max_length//len(chunks[:5]), \n",
    "                        min_length=min_length//len(chunks[:5]), \n",
    "                        do_sample=False\n",
    "                    )\n",
    "                    chunk_summaries.append(summary[0]['summary_text'])\n",
    "                \n",
    "                # Combine and re-summarize if needed\n",
    "                combined_summary = \" \".join(chunk_summaries)\n",
    "                if len(combined_summary) > max_length * 2:\n",
    "                    final_summary = self.summarizer(\n",
    "                        combined_summary,\n",
    "                        max_length=max_length,\n",
    "                        min_length=min_length,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "                    return final_summary[0]['summary_text']\n",
    "                else:\n",
    "                    return combined_summary\n",
    "            else:\n",
    "                # Direct summarization for shorter text\n",
    "                summary = self.summarizer(\n",
    "                    text,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                return summary[0]['summary_text']\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error during summarization: {str(e)}\"\n",
    "    \n",
    "    def process_url(self, url, summary_length=\"medium\"):\n",
    "        \"\"\"Main method to scrape URL and return summary\"\"\"\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "        \n",
    "        # Scrape content\n",
    "        content = self.scrape_content(url)\n",
    "        \n",
    "        if content.startswith(\"Error\"):\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"error\": content,\n",
    "                \"summary\": None,\n",
    "                \"original_length\": 0\n",
    "            }\n",
    "        \n",
    "        print(f\"Scraped {len(content)} characters\")\n",
    "        \n",
    "        # Set summary parameters based on desired length\n",
    "        length_params = {\n",
    "            \"short\": {\"max_length\": 1000, \"min_length\": 30},\n",
    "            \"medium\": {\"max_length\": 1000, \"min_length\": 50},\n",
    "            \"long\": {\"max_length\": 1000, \"min_length\": 100}\n",
    "        }\n",
    "        \n",
    "        params = length_params.get(summary_length, length_params[\"medium\"])\n",
    "        \n",
    "        print(\"Generating summary...\")\n",
    "        summary = self.summarize_text(content, **params)\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"summary\": summary,\n",
    "            \"original_length\": len(content),\n",
    "            \"summary_length\": len(summary),\n",
    "            \"compression_ratio\": f\"{len(summary)/len(content)*100:.1f}%\"\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "    scraper = WebScraperSummarizer()\n",
    "    \n",
    "    # Example URLs - replace with your target URLs\n",
    "    test_urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html#google_vignette\"\n",
    "    ]\n",
    "    \n",
    "    for url in test_urls:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        result = scraper.process_url(url, summary_length=\"medium\")\n",
    "        \n",
    "        if result.get(\"error\"):\n",
    "            print(f\"Failed to process {url}\")\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"URL: {result['url']}\")\n",
    "            print(f\"Original length: {result['original_length']} characters\")\n",
    "            print(f\"Summary length: {result['summary_length']} characters\")\n",
    "            print(f\"Compression: {result['compression_ratio']}\")\n",
    "            print(f\"\\nSummary:\\n{result['summary']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages first:\n",
    "    # pip install requests beautifulsoup4 transformers torch\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8405665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI model for summarization...\n",
      "Model loaded successfully!\n",
      "\\n============================================================\n",
      "Scraping content from: https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html\n",
      "Scraped 30283 characters\n",
      "Generating summary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 199. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=99)\n",
      "Your max_length is set to 200, but your input_length is only 97. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Your max_length is set to 200, but your input_length is only 145. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html\n",
      "Original length: 30283 characters\n",
      "Summary length: 1192 characters\n",
      "Compression: 3.9%\n",
      "\\nSummary:\\nWellness startup Biopeak raises $3.5 million in seed funding from Ranjan Pai office, Accel's Prashanth Prakash, others. Indian Rupee Oswal Pumps IPO Home Loan Interest Rates Sensex Live Inflation Forecast. Founded in 2024 by Rishi Pardal and Shiva Subramanian, Biopeak provides personalised health services through in-house clinics. Accel founding partner Prashanth Prakash, Claypond Capital (the family office of Manipal Group chairman Ranjan Pai), and NKSquared, the investment vehicle of Zerodha co-founder Nikhil Kamath. Biopeak combines molecular diagnostics, high-resolution imaging, and non-invasive testing to assess early signs of physiological changes. The company opened its first clinic in Bengaluru in March 2025. \"The moment is now to take control of your healthspan,\" Pardal said. Each client typically undergoes over six hours of multidisciplinary consultations. Biopeak primarily serves two categories of users: those seeking to improve overall health and performance, and individuals managing chronic or complex conditions that may not be adequately addressed by conventional treatment. The resulting health plan is revisited over time and refined through follow-up testing.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class WebScraperSummarizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the web scraper with AI summarization capabilities\"\"\"\n",
    "        print(\"Loading AI model for summarization...\")\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\", \n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            device=-1  # Use CPU\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def scrape_content(self, url):\n",
    "        \"\"\"Scrape text content from the given URL\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            for script in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "                script.decompose()\n",
    "\n",
    "            text_content = \"\"\n",
    "            content_selectors = [\n",
    "                'article', 'main', '.content', '#content', \n",
    "                '.post', '.entry', 'div.text', 'div.body'\n",
    "            ]\n",
    "\n",
    "            for selector in content_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    text_content = \" \".join([elem.get_text() for elem in elements])\n",
    "                    break\n",
    "\n",
    "            if not text_content:\n",
    "                paragraphs = soup.find_all(['p', 'div', 'span'])\n",
    "                text_content = \" \".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "            text_content = re.sub(r'\\\\s+', ' ', text_content).strip()\n",
    "            return text_content\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error fetching URL: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing content: {str(e)}\"\n",
    "\n",
    "    def chunk_text(self, text, max_chunk_size=1000):\n",
    "        \"\"\"Split text into chunks for processing\"\"\"\n",
    "        sentences = text.split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk + sentence) < max_chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def summarize_text(self, text, max_length=150, min_length=50):\n",
    "        \"\"\"Summarize text using AI model with professional prompt\"\"\"\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return \"Text too short to summarize meaningfully.\"\n",
    "\n",
    "        try:\n",
    "            instruction = \"Write a concise, professionally worded summary for a business news article: \"\n",
    "            prompt_text = instruction + text\n",
    "\n",
    "            if len(prompt_text) > 1000:\n",
    "                chunks = self.chunk_text(prompt_text)\n",
    "                chunk_summaries = []\n",
    "\n",
    "                for chunk in chunks[:5]:\n",
    "                    summary = self.summarizer(\n",
    "                        chunk,\n",
    "                        max_length=max_length//len(chunks[:5]),\n",
    "                        min_length=min_length//len(chunks[:5]),\n",
    "                        do_sample=False\n",
    "                    )\n",
    "                    chunk_summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "                combined_summary = \" \".join(chunk_summaries)\n",
    "                if len(combined_summary) > max_length * 2:\n",
    "                    final_summary = self.summarizer(\n",
    "                        combined_summary,\n",
    "                        max_length=max_length,\n",
    "                        min_length=min_length,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "                    return final_summary[0]['summary_text']\n",
    "                else:\n",
    "                    return combined_summary\n",
    "            else:\n",
    "                summary = self.summarizer(\n",
    "                    prompt_text,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                return summary[0]['summary_text']\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error during summarization: {str(e)}\"\n",
    "\n",
    "    def process_url(self, url, summary_length=\"medium\"):\n",
    "        \"\"\"Main method to scrape URL and return summary\"\"\"\n",
    "        print(f\"Scraping content from: {url}\")\n",
    "        content = self.scrape_content(url)\n",
    "\n",
    "        if content.startswith(\"Error\"):\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"error\": content,\n",
    "                \"summary\": None,\n",
    "                \"original_length\": 0\n",
    "            }\n",
    "\n",
    "        print(f\"Scraped {len(content)} characters\")\n",
    "\n",
    "        length_params = {\n",
    "            \"short\": {\"max_length\": 150, \"min_length\": 60},\n",
    "            \"medium\": {\"max_length\": 1000, \"min_length\": 100},\n",
    "            \"long\": {\"max_length\": 1000, \"min_length\": 150}\n",
    "        }\n",
    "\n",
    "        params = length_params.get(summary_length, length_params[\"medium\"])\n",
    "\n",
    "        print(\"Generating summary...\")\n",
    "        summary = self.summarize_text(content, **params)\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"summary\": summary,\n",
    "            \"original_length\": len(content),\n",
    "            \"summary_length\": len(summary),\n",
    "            \"compression_ratio\": f\"{len(summary)/len(content)*100:.1f}%\"\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "    scraper = WebScraperSummarizer()\n",
    "    test_urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/startup/wellness-startup-biopeak-raises-3-5-million-in-seed-funding-from-ranjan-pai-office-accel-s-prashanth-prakash-others-13103236.html\"\n",
    "    ]\n",
    "\n",
    "    for url in test_urls:\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        result = scraper.process_url(url, summary_length=\"medium\")\n",
    "\n",
    "        if result.get(\"error\"):\n",
    "            print(f\"Failed to process {url}\")\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"URL: {result['url']}\")\n",
    "            print(f\"Original length: {result['original_length']} characters\")\n",
    "            print(f\"Summary length: {result['summary_length']} characters\")\n",
    "            print(f\"Compression: {result['compression_ratio']}\")\n",
    "            print(f\"\\\\nSummary:\\\\n{result['summary']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
