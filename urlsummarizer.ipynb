{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4dc7ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Article Summarizer\n",
      "================================================================================\n",
      "Analyzing: https://www.livemint.com/market/stock-market-news/msci-rejig-swiggy-mazagon-dock-two-others-among-likely-additions-to-india-standard-index-in-august-rebalancing-11749890229453.html\n",
      "================================================================================\n",
      "Title: MSCI rejig: Swiggy, Mazagon Dock, two others among likely additions to India Standard Index in August rebalancing\n",
      "Authors: Ankit Gohel\n",
      "Publish Date: 2025-06-14 14:17:40+05:30\n",
      "Word Count: 296\n",
      "\n",
      "================================================== SUMMARY ==================================================\n",
      "\n",
      "ðŸ“° Newspaper3k Summary:\n",
      "------------------------------\n",
      "Swiggy, Mazagon Dock Shipbuilders, and two other stocks are expected to be added to the MSCI India Standard Index as part of the upcoming rebalancing scheduled for August 2025.\n",
      "The last MSCI rebalancing was conducted on May 14, wherein the Coromandel International and FSN E-commerce Ventures, the parent company of the fashion and beauty e-tailer Nykaa, were included in the MSCI India Index, which is part of the MSCI Global Standard Index.\n",
      "The MSCI India Standard Index captures the performance of the large- and mid-cap segments of the Indian equity market, covering approximately 85% of the investable universe.\n",
      "If included in MSCI India Standard Index, Swiggy is expected to receive estimated inflows of $385 million, with 93.8 million shares being added to the index.\n",
      "Hitachi Energy India shares have delivered 11% returns, and Waaree Energies shares have gained around 5% during the same period.\n",
      "\n",
      "ðŸ”¤ TextBlob Summary (3 sentences):\n",
      "------------------------------\n",
      "Swiggy, Mazagon Dock Shipbuilders, and two other stocks are expected to be added to the MSCI India Standard Index as part of the upcoming rebalancing scheduled for August 2025. According to JM Financial, the August rebalancing could see as many as four inclusions, potentially drawing an estimated $850 million in passive inflows. If included in MSCI India Standard Index, Swiggy is expected to receive estimated inflows of $385 million, with 93.8 million shares being added to the index.\n",
      "\n",
      "ðŸ’­ Sentiment Analysis:\n",
      "------------------------------\n",
      "Sentiment: Neutral (Polarity: 0.01)\n",
      "Tone: Objective (Subjectivity: 0.28)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from newspaper import Article\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK data\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "def generate_news_summary(url, summary_sentences=3):\n",
    "    \"\"\"\n",
    "    Generate a summary from a news article URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The news article URL\n",
    "        summary_sentences (int): Number of sentences in the summary (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains title, summary, sentiment, and other article info\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate URL\n",
    "        parsed_url = urlparse(url)\n",
    "        if not parsed_url.scheme or not parsed_url.netloc:\n",
    "            raise ValueError(\"Invalid URL format\")\n",
    "        \n",
    "        # Initialize and download article\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        # Check if article was successfully parsed\n",
    "        if not article.text:\n",
    "            raise ValueError(\"Could not extract text from the article\")\n",
    "        \n",
    "        # Use newspaper3k's built-in summarization\n",
    "        article.nlp()\n",
    "        newspaper_summary = article.summary\n",
    "        \n",
    "        # Alternative summary using TextBlob and NLTK\n",
    "        blob = TextBlob(article.text)\n",
    "        sentences = blob.sentences\n",
    "        \n",
    "        # Simple extractive summarization - get top sentences\n",
    "        if len(sentences) <= summary_sentences:\n",
    "            textblob_summary = str(blob)\n",
    "        else:\n",
    "            # Get sentences from different parts of the article\n",
    "            step = len(sentences) // summary_sentences\n",
    "            selected_sentences = []\n",
    "            for i in range(0, len(sentences), step):\n",
    "                if len(selected_sentences) < summary_sentences:\n",
    "                    selected_sentences.append(str(sentences[i]))\n",
    "            textblob_summary = ' '.join(selected_sentences)\n",
    "        \n",
    "        # Sentiment analysis using TextBlob\n",
    "        sentiment = blob.sentiment\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'url': url,\n",
    "            'title': article.title,\n",
    "            'authors': article.authors,\n",
    "            'publish_date': article.publish_date,\n",
    "            'newspaper_summary': newspaper_summary,\n",
    "            'textblob_summary': textblob_summary,\n",
    "            'sentiment': {\n",
    "                'polarity': sentiment.polarity,  # -1 (negative) to 1 (positive)\n",
    "                'subjectivity': sentiment.subjectivity  # 0 (objective) to 1 (subjective)\n",
    "            },\n",
    "            'word_count': len(article.text.split()),\n",
    "            'top_image': article.top_image\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'url': url\n",
    "        }\n",
    "\n",
    "def print_summary(url, summary_sentences=3):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of a news article\n",
    "    \n",
    "    Args:\n",
    "        url (str): The news article URL\n",
    "        summary_sentences (int): Number of sentences in the summary\n",
    "    \"\"\"\n",
    "    # Setup NLTK if needed\n",
    "    setup_nltk()\n",
    "    \n",
    "    print(f\"Analyzing: {url}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    result = generate_news_summary(url, summary_sentences)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Authors: {', '.join(result['authors']) if result['authors'] else 'Unknown'}\")\n",
    "    print(f\"Publish Date: {result['publish_date']}\")\n",
    "    print(f\"Word Count: {result['word_count']}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \" SUMMARY \" + \"=\"*50)\n",
    "    \n",
    "    # Print newspaper3k summary\n",
    "    print(\"\\nðŸ“° Newspaper3k Summary:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(result['newspaper_summary'])\n",
    "    \n",
    "    # Print TextBlob summary\n",
    "    print(f\"\\nðŸ”¤ TextBlob Summary ({summary_sentences} sentences):\")\n",
    "    print(\"-\" * 30)\n",
    "    print(result['textblob_summary'])\n",
    "    \n",
    "    # Print sentiment analysis\n",
    "    print(f\"\\nðŸ’­ Sentiment Analysis:\")\n",
    "    print(\"-\" * 30)\n",
    "    polarity = result['sentiment']['polarity']\n",
    "    subjectivity = result['sentiment']['subjectivity']\n",
    "    \n",
    "    sentiment_label = \"Neutral\"\n",
    "    if polarity > 0.1:\n",
    "        sentiment_label = \"Positive\"\n",
    "    elif polarity < -0.1:\n",
    "        sentiment_label = \"Negative\"\n",
    "    \n",
    "    objectivity_label = \"Objective\" if subjectivity < 0.5 else \"Subjective\"\n",
    "    \n",
    "    print(f\"Sentiment: {sentiment_label} (Polarity: {polarity:.2f})\")\n",
    "    print(f\"Tone: {objectivity_label} (Subjectivity: {subjectivity:.2f})\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URLs - replace with actual news URLs\n",
    "    test_urls = [\n",
    "        \"https://www.bbc.com/news/technology-12345678\",  # Replace with real URL\n",
    "        \"https://www.cnn.com/2024/01/01/tech/example-news/index.html\"  # Replace with real URL\n",
    "    ]\n",
    "    \n",
    "    # Test with a single URL\n",
    "    print(\"News Article Summarizer\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get URL from user input\n",
    "    url = input(\"Enter a news article URL: \")\n",
    "    if url.strip():\n",
    "        print_summary(url.strip())\n",
    "    else:\n",
    "        print(\"No URL provided. Please run the script again with a valid news URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e97ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\faiza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b50cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Interactive News Summarizer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from newspaper import Article\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK data\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "def get_news_summary(url, summary_sentences=3, return_json=True):\n",
    "    \"\"\"\n",
    "    Generate a summary from a news article URL and return as JSON\n",
    "    \n",
    "    Args:\n",
    "        url (str): The news article URL\n",
    "        summary_sentences (int): Number of sentences in the summary (default: 3)\n",
    "        return_json (bool): If True, returns JSON string; if False, returns dict\n",
    "    \n",
    "    Returns:\n",
    "        str or dict: JSON string or dictionary containing article summary and metadata\n",
    "    \"\"\"\n",
    "    # Setup NLTK if needed\n",
    "    setup_nltk()\n",
    "    \n",
    "    try:\n",
    "        # Validate URL\n",
    "        parsed_url = urlparse(url)\n",
    "        if not parsed_url.scheme or not parsed_url.netloc:\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error': 'Invalid URL format',\n",
    "                'url': url,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "        \n",
    "        # Initialize and download article\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        # Check if article was successfully parsed\n",
    "        if not article.text:\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error': 'Could not extract text from the article',\n",
    "                'url': url,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "        \n",
    "        # Use newspaper3k's built-in summarization\n",
    "        article.nlp()\n",
    "        newspaper_summary = article.summary\n",
    "        \n",
    "        # Alternative summary using TextBlob and NLTK\n",
    "        blob = TextBlob(article.text)\n",
    "        sentences = blob.sentences\n",
    "        \n",
    "        # Simple extractive summarization - get top sentences\n",
    "        if len(sentences) <= summary_sentences:\n",
    "            textblob_summary = str(blob)\n",
    "        else:\n",
    "            # Get sentences from different parts of the article\n",
    "            step = len(sentences) // summary_sentences\n",
    "            selected_sentences = []\n",
    "            for i in range(0, len(sentences), step):\n",
    "                if len(selected_sentences) < summary_sentences:\n",
    "                    selected_sentences.append(str(sentences[i]))\n",
    "            textblob_summary = ' '.join(selected_sentences)\n",
    "        \n",
    "        # Sentiment analysis using TextBlob\n",
    "        sentiment = blob.sentiment\n",
    "        \n",
    "        # Determine sentiment label\n",
    "        sentiment_label = \"neutral\"\n",
    "        if sentiment.polarity > 0.1:\n",
    "            sentiment_label = \"positive\"\n",
    "        elif sentiment.polarity < -0.1:\n",
    "            sentiment_label = \"negative\"\n",
    "        \n",
    "        # Determine objectivity label\n",
    "        objectivity_label = \"objective\" if sentiment.subjectivity < 0.5 else \"subjective\"\n",
    "        \n",
    "        # Prepare successful result\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'url': url,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'article': {\n",
    "                'title': article.title or 'No title found',\n",
    "                'authors': article.authors or [],\n",
    "                'publish_date': article.publish_date.isoformat() if article.publish_date else None,\n",
    "                'word_count': len(article.text.split()),\n",
    "                'top_image': article.top_image or None\n",
    "            },\n",
    "            'summaries': {\n",
    "                'newspaper3k': newspaper_summary or 'No summary generated',\n",
    "                'textblob': textblob_summary or 'No summary generated'\n",
    "            },\n",
    "            'sentiment_analysis': {\n",
    "                'polarity': round(sentiment.polarity, 3),\n",
    "                'subjectivity': round(sentiment.subjectivity, 3),\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'objectivity_label': objectivity_label\n",
    "            },\n",
    "            'keywords': article.keywords[:10] if hasattr(article, 'keywords') and article.keywords else []\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2, default=str) if return_json else result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_result = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'url': url,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the function\n",
    "    test_url = \"https://www.bbc.com/news/technology\"  # Replace with a real news URL\n",
    "    \n",
    "    # Get JSON result\n",
    "    json_result = get_news_summary(test_url)\n",
    "    print(\"JSON Result:\")\n",
    "    print(json_result)\n",
    "    \n",
    "    # Get dict result (for programmatic use)\n",
    "    dict_result = get_news_summary(`test_url`, return_json=False)\n",
    "    print(\"\\nDict Result (success status):\", dict_result.get('success'))\n",
    "    \n",
    "    # Interactive example\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Interactive News Summarizer\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    url = input(\"Enter a news article URL (or press Enter to skip): \")\n",
    "    if url.strip():\n",
    "        result = get_news_summary(url.strip())\n",
    "        print(\"\\nResult:\")\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "441c3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"success\": true,\n",
      "  \"url\": \"https://www.financialexpress.com/business/brandwagon-homeessentials-raises-2-2m-from-india-quotient-plans-omnichannel-expansion-3878977\",\n",
      "  \"timestamp\": \"2025-06-16T11:18:15.118398\",\n",
      "  \"article\": {\n",
      "    \"title\": \"HomeEssentials raises $2.2M from India Quotient, plans omnichannel expansion\",\n",
      "    \"authors\": [\n",
      "      \"Brandwagon Online\"\n",
      "    ],\n",
      "    \"publish_date\": null,\n",
      "    \"word_count\": 281,\n",
      "    \"top_image\": \"https://www.financialexpress.com/wp-content/uploads/2025/06/life-47.png\"\n",
      "  },\n",
      "  \"summaries\": {\n",
      "    \"newspaper3k\": \"HomeEssentials, a home and kitchen brand known for its modern, multi-use products, has raised $2.2 million in funding from early-stage venture capital firm India Quotient.\\nThe company plans to use the capital to expand into offline retail, aiming to merge its digital success with a physical footprint across metros and smaller cities.\\nFounded by entrepreneurs Tanishq and Divyam, the brand offers more than 250 products designed with utility, affordability and aesthetics in mind.\\nThe founders said their offline push is a strategic move to deepen market reach and build stronger consumer trust through in-store experiences.\\nNow, with plans for high-visibility retail locations in Tier 1, 2 and 3 cities, the brand seeks to provide a more tactile experience to consumers.\",\n",
      "    \"textblob\": \"HomeEssentials, a home and kitchen brand known for its modern, multi-use products, has raised $2.2 million in funding from early-stage venture capital firm India Quotient. The founders said their offline push is a strategic move to deepen market reach and build stronger consumer trust through in-store experiences. The startup\\u2019s emphasis on design-led functionality has resonated with online shoppers.\"\n",
      "  },\n",
      "  \"sentiment_analysis\": {\n",
      "    \"polarity\": 0.163,\n",
      "    \"subjectivity\": 0.278,\n",
      "    \"sentiment_label\": \"positive\",\n",
      "    \"objectivity_label\": \"objective\"\n",
      "  },\n",
      "  \"keywords\": [\n",
      "    \"raises\",\n",
      "    \"market\",\n",
      "    \"raised\",\n",
      "    \"quotient\",\n",
      "    \"plans\",\n",
      "    \"brand\",\n",
      "    \"expansion\",\n",
      "    \"reach\",\n",
      "    \"india\",\n",
      "    \"homeessentials\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_result = get_news_summary(\"https://www.financialexpress.com/business/brandwagon-homeessentials-raises-2-2m-from-india-quotient-plans-omnichannel-expansion-3878977\")\n",
    "print(json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a197557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this as redirect_resolver.py or in a cell\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def resolve_redirected_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a Google News redirect URL, resolves and returns the final redirected URL.\n",
    "    \"\"\"\n",
    "    # Setup Chrome options (headless)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Start WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the redirect to complete\n",
    "        final_url = driver.current_url\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return final_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a6b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Result:\n",
      "{\n",
      "  \"success\": false,\n",
      "  \"error\": \"Could not extract text from the article\",\n",
      "  \"url\": \"https://www.business-standard.com/markets/news/msci-rejig-swiggy-mazagon-dock-among-4-entrants-850-mn-inflows-likely-125061600250_1.html\",\n",
      "  \"timestamp\": \"2025-06-16T11:52:36.714758\"\n",
      "}\n",
      "\n",
      "Dict Result (success status): False\n",
      "\n",
      "============================================================\n",
      "Interactive News Summarizer\n",
      "============================================================\n",
      "\n",
      "Result:\n",
      "{\n",
      "  \"success\": false,\n",
      "  \"error\": \"Could not extract text from the article\",\n",
      "  \"url\": \"https://www.business-standard.com/markets/news/msci-rejig-swiggy-mazagon-dock-among-4-entrants-850-mn-inflows-likely-125061600250_1.html\",\n",
      "  \"timestamp\": \"2025-06-16T11:53:03.413878\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from newspaper import Article\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK data\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "def resolve_redirected_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a Google News redirect URL, resolves and returns the final redirected URL.\n",
    "    \"\"\"\n",
    "    # Setup Chrome options (headless)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Start WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the redirect to complete\n",
    "        final_url = driver.current_url\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return final_url\n",
    "\n",
    "\n",
    "def get_news_summary(url, summary_sentences=3, return_json=True):\n",
    "    \"\"\"\n",
    "    Generate a summary from a news article URL and return as JSON\n",
    "    \n",
    "    Args:\n",
    "        url (str): The news article URL\n",
    "        summary_sentences (int): Number of sentences in the summary (default: 3)\n",
    "        return_json (bool): If True, returns JSON string; if False, returns dict\n",
    "    \n",
    "    Returns:\n",
    "        str or dict: JSON string or dictionary containing article summary and metadata\n",
    "    \"\"\"\n",
    "    # Setup NLTK if needed\n",
    "    setup_nltk()\n",
    "    final_url = resolve_redirected_url(url)\n",
    "    try:\n",
    "        # Validate URL\n",
    "        parsed_url = urlparse(final_url)\n",
    "        if not parsed_url.scheme or not parsed_url.netloc:\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error': 'Invalid URL format',\n",
    "                'url': final_url,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "        \n",
    "        # Initialize and download article\n",
    "        article = Article(final_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        # Check if article was successfully parsed\n",
    "        if not article.text:\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error': 'Could not extract text from the article',\n",
    "                'url': final_url,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "        \n",
    "        # Use newspaper3k's built-in summarization\n",
    "        article.nlp()\n",
    "        newspaper_summary = article.summary\n",
    "        \n",
    "        # Alternative summary using TextBlob and NLTK\n",
    "        blob = TextBlob(article.text)\n",
    "        sentences = blob.sentences\n",
    "        \n",
    "        # Simple extractive summarization - get top sentences\n",
    "        if len(sentences) <= summary_sentences:\n",
    "            textblob_summary = str(blob)\n",
    "        else:\n",
    "            # Get sentences from different parts of the article\n",
    "            step = len(sentences) // summary_sentences\n",
    "            selected_sentences = []\n",
    "            for i in range(0, len(sentences), step):\n",
    "                if len(selected_sentences) < summary_sentences:\n",
    "                    selected_sentences.append(str(sentences[i]))\n",
    "            textblob_summary = ' '.join(selected_sentences)\n",
    "        \n",
    "        # Sentiment analysis using TextBlob\n",
    "        sentiment = blob.sentiment\n",
    "        \n",
    "        # Determine sentiment label\n",
    "        sentiment_label = \"neutral\"\n",
    "        if sentiment.polarity > 0.1:\n",
    "            sentiment_label = \"positive\"\n",
    "        elif sentiment.polarity < -0.1:\n",
    "            sentiment_label = \"negative\"\n",
    "        \n",
    "        # Determine objectivity label\n",
    "        objectivity_label = \"objective\" if sentiment.subjectivity < 0.5 else \"subjective\"\n",
    "        \n",
    "        # Prepare successful result\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'url': url,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'article': {\n",
    "                'title': article.title or 'No title found',\n",
    "                'authors': article.authors or [],\n",
    "                'publish_date': article.publish_date.isoformat() if article.publish_date else None,\n",
    "                'word_count': len(article.text.split()),\n",
    "                'top_image': article.top_image or None\n",
    "            },\n",
    "            'summaries': {\n",
    "                'newspaper3k': newspaper_summary or 'No summary generated',\n",
    "                'textblob': textblob_summary or 'No summary generated'\n",
    "            },\n",
    "            'sentiment_analysis': {\n",
    "                'polarity': round(sentiment.polarity, 3),\n",
    "                'subjectivity': round(sentiment.subjectivity, 3),\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'objectivity_label': objectivity_label\n",
    "            },\n",
    "            'keywords': article.keywords[:10] if hasattr(article, 'keywords') and article.keywords else []\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2, default=str) if return_json else result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_result = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'url': url,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the function\n",
    "    test_url = \"https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0?hl=en-IN&gl=IN&ceid=IN%3Aen\"  # Replace with a real news URL\n",
    "    \n",
    "    # Get JSON result\n",
    "    json_result = get_news_summary(test_url)\n",
    "    print(\"JSON Result:\")\n",
    "    print(json_result)\n",
    "    \n",
    "    # Get dict result (for programmatic use)\n",
    "    dict_result = get_news_summary(test_url, return_json=False)\n",
    "    print(\"\\nDict Result (success status):\", dict_result.get('success'))\n",
    "    \n",
    "    # Interactive example\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Interactive News Summarizer\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    url = input(\"Enter a news article URL (or press Enter to skip): \")\n",
    "    if url.strip():\n",
    "        result = get_news_summary(url.strip())\n",
    "        print(\"\\nResult:\")\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fd39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redirect_resolver import resolve_redirected_url\n",
    "\n",
    "url = \"https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0?hl=en-IN&gl=IN&ceid=IN%3Aen\"  # your Google News URL\n",
    "final_url = resolve_redirected_url(url)\n",
    "print(\"Final URL:\", final_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2139a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Comprehensive News Scraper (Using External Summary Function)\n",
      "======================================================================\n",
      "Note: Using external get_news_summary function for content extraction and summarization\n",
      "\n",
      "Starting comprehensive scraping...\n",
      "Starting comprehensive news scraping for: swiggy\n",
      "Using external get_news_summary function\n",
      "================================================================================\n",
      "Searching for news articles about: swiggy\n",
      "URL: https://news.google.com/search?q=swiggy&hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "--------------------------------------------------\n",
      "Found Article 1: 'Not out of need, but to rebuild ties': Bengaluru techie uses Swiggy deliveries to meet people, find clients\n",
      "Found Article 2: MSCI rejig: Swiggy, Mazagon Dock, two others among likely additions to India Standard Index in August rebalancing\n",
      "Found Article 3: MSCI rejig: Swiggy, Mazagon Dock among 4 entrants; $850 mn inflows likely\n",
      "\n",
      "Processing article 1/3\n",
      "--------------------------------------------------\n",
      "Actual URL: https://news.google.com/read/CBMijAJBVV95cUxNTnBWX19WVVUwSDBOQm10enpoMi1OUmpReXBGR2FtbGtmYkQxX2F1TkVwcl9iWXVuN3YyOS1ZRU05VTlKZTRVLUFaZFN5b0VFR19pV1E5VU80RFU4WENHeXE4Um5HdHRiSGd6NE5UVUtpd05wbEN6aG52YUZBdnQ1RFlKSkRyRWR4YVNkUTMzRW85bUZsNW5VVHFvYWd2WWtaOGVnRFMxQ0ZkQUxwMWs5WHRGdUZoTm41OGRRbFNwRk02SEhwbkh1ZklrSXRHbGVxZXprbWloVXhZeDh0dzR5a0NTYmtjdkpfa0M2ZlRaYUpmeXZzOU1OUkxyMWo1WFJrN0E4MXdHZ29KODNm?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Getting summary from external function for: https://news.google.com/read/CBMijAJBVV95cUxNTnBWX19WVVUwSDBOQm10enpoMi1OUmpReXBGR2FtbGtmYkQxX2F1TkVwcl9iWXVuN3YyOS1ZRU05VTlKZTRVLUFaZFN5b0VFR19pV1E5VU80RFU4WENHeXE4Um5HdHRiSGd6NE5UVUtpd05wbEN6aG52YUZBdnQ1RFlKSkRyRWR4YVNkUTMzRW85bUZsNW5VVHFvYWd2WWtaOGVnRFMxQ0ZkQUxwMWs5WHRGdUZoTm41OGRRbFNwRk02SEhwbkh1ZklrSXRHbGVxZXprbWloVXhZeDh0dzR5a0NTYmtjdkpfa0M2ZlRaYUpmeXZzOU1OUkxyMWo1WFJrN0E4MXdHZ29KODNm?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Calling get_news_summary for: https://news.google.com/read/CBMijAJBVV95cUxNTnBWX19WVVUwSDBOQm10enpoMi1OUmpReXBGR2FtbGtmYkQxX2F1TkVwcl9iWXVuN3YyOS1ZRU05VTlKZTRVLUFaZFN5b0VFR19pV1E5VU80RFU4WENHeXE4Um5HdHRiSGd6NE5UVUtpd05wbEN6aG52YUZBdnQ1RFlKSkRyRWR4YVNkUTMzRW85bUZsNW5VVHFvYWd2WWtaOGVnRFMxQ0ZkQUxwMWs5WHRGdUZoTm41OGRRbFNwRk02SEhwbkh1ZklrSXRHbGVxZXprbWloVXhZeDh0dzR5a0NTYmtjdkpfa0M2ZlRaYUpmeXZzOU1OUkxyMWo1WFJrN0E4MXdHZ29KODNm?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Resolved URL: https://timesofindia.indiatimes.com/city/bengaluru/not-out-of-need-but-to-rebuild-ties-bengaluru-techie-uses-swiggy-deliveries-to-meet-people-find-clients/articleshow/121872275.cms\n",
      "âœ“ Successfully extracted summary using external function\n",
      "âœ“ Newspaper3k Summary: BENGALURU: In a country where tech professionals usually seek the safety net of salaried jobs after ...\n",
      "\n",
      "Processing article 2/3\n",
      "--------------------------------------------------\n",
      "Actual URL: https://news.google.com/read/CBMijAJBVV95cUxPOWVFMURLZ2RGY0N1Wk5sT0tTS3NhcHJEYUJOYW1abEtpVzJTZG1pcUFpUlBaNWVEQ3UxQlh2QURJX01ld0ZEbWFSeEFtOU1Sb0Y0NV9XaW54UnY1OFV1Y2RWdmZ6ZkRyMVlWTEo5eWh2N0F4d1NRZlo1UnVvMmZzdzdYY2t0OHRLcGxkaHB4LWJZVHBTTThQRGJMQnA4MEZMRFNLdjlrZmdHYlNwSm1aeDViWVFlXzNPZ1g1Xy04UlV2NnZvOGczb2x5U2tFeGNMekpBNXl1V3lBNE1GbTgyUDZkV0tFUmNBNUVMWWhtYVNvdTEwNHhfcGFUVGxKQ0hYTXFSWjhyOGx2TEhQ0gGSAkFVX3lxTE9oQXN4UHczTngzNTEwUmhwanNmMjkzSG9WVzRNOFR0cktkYVVSYjg3ZUJfX2xobldUWFVKdXFGbWhYRnJjNWRCMDlMWmdsc19FSzBEQlY1eENFdElKN241b1Q1a3VmVTNJb2IyYzJvd1pyN2JIdXl0MEctMldsT1p3MXFNQ0YyUlRScWdmWTNkR2I4dUxadUxhZ2hKQUZmLVFHZTR4TWZIaXRKNUw5OXB0cmRpdzdaUDdhaDZHMjJRdkRDS2hmTTNmNzhmWXhfZHJmcTdtVTM5NnQwMG5Na2kzME5EajJZX2tOMlNqcjhOekdsX2xVZU1KSWNDVlU4aWJOa05TeGR2akU0UVJ2WWNTTWc?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Getting summary from external function for: https://news.google.com/read/CBMijAJBVV95cUxPOWVFMURLZ2RGY0N1Wk5sT0tTS3NhcHJEYUJOYW1abEtpVzJTZG1pcUFpUlBaNWVEQ3UxQlh2QURJX01ld0ZEbWFSeEFtOU1Sb0Y0NV9XaW54UnY1OFV1Y2RWdmZ6ZkRyMVlWTEo5eWh2N0F4d1NRZlo1UnVvMmZzdzdYY2t0OHRLcGxkaHB4LWJZVHBTTThQRGJMQnA4MEZMRFNLdjlrZmdHYlNwSm1aeDViWVFlXzNPZ1g1Xy04UlV2NnZvOGczb2x5U2tFeGNMekpBNXl1V3lBNE1GbTgyUDZkV0tFUmNBNUVMWWhtYVNvdTEwNHhfcGFUVGxKQ0hYTXFSWjhyOGx2TEhQ0gGSAkFVX3lxTE9oQXN4UHczTngzNTEwUmhwanNmMjkzSG9WVzRNOFR0cktkYVVSYjg3ZUJfX2xobldUWFVKdXFGbWhYRnJjNWRCMDlMWmdsc19FSzBEQlY1eENFdElKN241b1Q1a3VmVTNJb2IyYzJvd1pyN2JIdXl0MEctMldsT1p3MXFNQ0YyUlRScWdmWTNkR2I4dUxadUxhZ2hKQUZmLVFHZTR4TWZIaXRKNUw5OXB0cmRpdzdaUDdhaDZHMjJRdkRDS2hmTTNmNzhmWXhfZHJmcTdtVTM5NnQwMG5Na2kzME5EajJZX2tOMlNqcjhOekdsX2xVZU1KSWNDVlU4aWJOa05TeGR2akU0UVJ2WWNTTWc?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Calling get_news_summary for: https://news.google.com/read/CBMijAJBVV95cUxPOWVFMURLZ2RGY0N1Wk5sT0tTS3NhcHJEYUJOYW1abEtpVzJTZG1pcUFpUlBaNWVEQ3UxQlh2QURJX01ld0ZEbWFSeEFtOU1Sb0Y0NV9XaW54UnY1OFV1Y2RWdmZ6ZkRyMVlWTEo5eWh2N0F4d1NRZlo1UnVvMmZzdzdYY2t0OHRLcGxkaHB4LWJZVHBTTThQRGJMQnA4MEZMRFNLdjlrZmdHYlNwSm1aeDViWVFlXzNPZ1g1Xy04UlV2NnZvOGczb2x5U2tFeGNMekpBNXl1V3lBNE1GbTgyUDZkV0tFUmNBNUVMWWhtYVNvdTEwNHhfcGFUVGxKQ0hYTXFSWjhyOGx2TEhQ0gGSAkFVX3lxTE9oQXN4UHczTngzNTEwUmhwanNmMjkzSG9WVzRNOFR0cktkYVVSYjg3ZUJfX2xobldUWFVKdXFGbWhYRnJjNWRCMDlMWmdsc19FSzBEQlY1eENFdElKN241b1Q1a3VmVTNJb2IyYzJvd1pyN2JIdXl0MEctMldsT1p3MXFNQ0YyUlRScWdmWTNkR2I4dUxadUxhZ2hKQUZmLVFHZTR4TWZIaXRKNUw5OXB0cmRpdzdaUDdhaDZHMjJRdkRDS2hmTTNmNzhmWXhfZHJmcTdtVTM5NnQwMG5Na2kzME5EajJZX2tOMlNqcjhOekdsX2xVZU1KSWNDVlU4aWJOa05TeGR2akU0UVJ2WWNTTWc?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Resolved URL: https://www.livemint.com/market/stock-market-news/msci-rejig-swiggy-mazagon-dock-two-others-among-likely-additions-to-india-standard-index-in-august-rebalancing-11749890229453.html\n",
      "âœ“ Successfully extracted summary using external function\n",
      "âœ“ Newspaper3k Summary: Swiggy, Mazagon Dock Shipbuilders, and two other stocks are expected to be added to the MSCI India S...\n",
      "\n",
      "Processing article 3/3\n",
      "--------------------------------------------------\n",
      "Actual URL: https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Getting summary from external function for: https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Calling get_news_summary for: https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Resolved URL: https://www.business-standard.com/markets/news/msci-rejig-swiggy-mazagon-dock-among-4-entrants-850-mn-inflows-likely-125061600250_1.html\n",
      "âœ— Failed to get summary from external function\n",
      "{'success': False, 'error': 'Could not extract text from the article', 'url': 'https://www.business-standard.com/markets/news/msci-rejig-swiggy-mazagon-dock-among-4-entrants-850-mn-inflows-likely-125061600250_1.html', 'timestamp': '2025-06-16T12:01:52.262073'}\n",
      "âœ— Failed to extract content using external function\n",
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE NEWS SCRAPING SUMMARY (Using External Summary Function)\n",
      "====================================================================================================\n",
      "Search Query: swiggy\n",
      "Scraped At: 2025-06-16T12:01:54.263103\n",
      "Total Articles Found: 3\n",
      "Successful External Extractions: 2\n",
      "Summary Method: external_get_news_summary_function\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Article 1:\n",
      "  Title: 'Not out of need, but to rebuild ties': Bengaluru techie uses Swiggy deliveries to meet people, find clients\n",
      "  Source: Times of India\n",
      "  Date: 3 hours ago\n",
      "  External Summary Extraction: âœ“\n",
      "  Newspaper3k Summary: BENGALURU: In a country where tech professionals usually seek the safety net of salaried jobs after entrepreneurial setbacks, Padmanaban Ebbas (40) is...\n",
      "  Word Count: 250\n",
      "  Keywords: rebuild, ties, salaried, post, tech\n",
      "  Sentiment: positive (polarity: 0.14)\n",
      "\n",
      "Article 2:\n",
      "  Title: MSCI rejig: Swiggy, Mazagon Dock, two others among likely additions to India Standard Index in August rebalancing\n",
      "  Source: Mint\n",
      "  Date: 2 days ago\n",
      "  External Summary Extraction: âœ“\n",
      "  Newspaper3k Summary: Swiggy, Mazagon Dock Shipbuilders, and two other stocks are expected to be added to the MSCI India Standard Index as part of the upcoming rebalancing ...\n",
      "  Word Count: 296\n",
      "  Keywords: dock, estimated, index, rebalancing, shares\n",
      "  Sentiment: neutral (polarity: 0.012)\n",
      "\n",
      "Article 3:\n",
      "  Title: MSCI rejig: Swiggy, Mazagon Dock among 4 entrants; $850 mn inflows likely\n",
      "  Source: Business Standard\n",
      "  Date: 50 minutes ago\n",
      "  External Summary Extraction: âœ—\n",
      "====================================================================================================\n",
      "\n",
      "Comprehensive news data saved to: swiggyupdated.json\n",
      "Data saved to: swiggyupdated.json\n",
      "\n",
      "Scraping completed using external summary function!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Import the news summary function\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from newspaper import Article\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Add the get_news_summary function\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK data\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "def resolve_redirected_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a Google News redirect URL, resolves and returns the final redirected URL.\n",
    "    \"\"\"\n",
    "    # Setup Chrome options (headless)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Start WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the redirect to complete\n",
    "        final_url = driver.current_url\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return final_url\n",
    "\n",
    "\n",
    "def get_news_summary(url, summary_sentences=3, return_json=False):\n",
    "    \"\"\"\n",
    "    Generate a summary from a news article URL and return as dict\n",
    "    \n",
    "    Args:\n",
    "        url (str): The news article URL\n",
    "        summary_sentences (int): Number of sentences in the summary (default: 3)\n",
    "        return_json (bool): If True, returns JSON string; if False, returns dict\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing article summary and metadata\n",
    "    \"\"\"\n",
    "    # Setup NLTK if needed\n",
    "    setup_nltk()\n",
    "    final_url = resolve_redirected_url(url)\n",
    "    print(f\"Resolved URL: {final_url}\")\n",
    "    try:\n",
    "        # Validate URL\n",
    "        parsed_url = urlparse(final_url)\n",
    "        if not parsed_url.scheme or not parsed_url.netloc:\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error': 'Invalid URL format',\n",
    "                'url': final_url,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "        \n",
    "        # Initialize and download article\n",
    "        article = Article(final_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        # Check if article was successfully parsed\n",
    "        if not article.text:\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error': 'Could not extract text from the article',\n",
    "                'url': final_url,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "        \n",
    "        # Use newspaper3k's built-in summarization\n",
    "        article.nlp()\n",
    "        newspaper_summary = article.summary\n",
    "        \n",
    "        # Alternative summary using TextBlob and NLTK\n",
    "        blob = TextBlob(article.text)\n",
    "        sentences = blob.sentences\n",
    "        \n",
    "        # Simple extractive summarization - get top sentences\n",
    "        if len(sentences) <= summary_sentences:\n",
    "            textblob_summary = str(blob)\n",
    "        else:\n",
    "            # Get sentences from different parts of the article\n",
    "            step = len(sentences) // summary_sentences\n",
    "            selected_sentences = []\n",
    "            for i in range(0, len(sentences), step):\n",
    "                if len(selected_sentences) < summary_sentences:\n",
    "                    selected_sentences.append(str(sentences[i]))\n",
    "            textblob_summary = ' '.join(selected_sentences)\n",
    "        \n",
    "        # Sentiment analysis using TextBlob\n",
    "        sentiment = blob.sentiment\n",
    "        \n",
    "        # Determine sentiment label\n",
    "        sentiment_label = \"neutral\"\n",
    "        if sentiment.polarity > 0.1:\n",
    "            sentiment_label = \"positive\"\n",
    "        elif sentiment.polarity < -0.1:\n",
    "            sentiment_label = \"negative\"\n",
    "        \n",
    "        # Determine objectivity label\n",
    "        objectivity_label = \"objective\" if sentiment.subjectivity < 0.5 else \"subjective\"\n",
    "        \n",
    "        # Prepare successful result\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'url': final_url,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'article': {\n",
    "                'title': article.title or 'No title found',\n",
    "                'authors': article.authors or [],\n",
    "                'publish_date': article.publish_date.isoformat() if article.publish_date else None,\n",
    "                'word_count': len(article.text.split()),\n",
    "                'top_image': article.top_image or None\n",
    "            },\n",
    "            'summaries': {\n",
    "                'newspaper3k': newspaper_summary or 'No summary generated',\n",
    "                'textblob': textblob_summary or 'No summary generated'\n",
    "            },\n",
    "            'sentiment_analysis': {\n",
    "                'polarity': round(sentiment.polarity, 3),\n",
    "                'subjectivity': round(sentiment.subjectivity, 3),\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'objectivity_label': objectivity_label\n",
    "            },\n",
    "            'keywords': article.keywords[:10] if hasattr(article, 'keywords') and article.keywords else []\n",
    "        }\n",
    "        \n",
    "        return json.dumps(result, indent=2, default=str) if return_json else result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_result = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'url': final_url,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        return json.dumps(error_result, indent=2) if return_json else error_result\n",
    "\n",
    "class ComprehensiveNewsScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the comprehensive news scraper\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        \n",
    "        # Headers for requests\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "\n",
    "    def scrape_google_news_articles(self, company_name, max_articles=5):\n",
    "        \"\"\"\n",
    "        Scrapes Google News for articles about a specific company\n",
    "        \n",
    "        Args:\n",
    "            company_name (str): Name of the company to search for\n",
    "            max_articles (int): Maximum number of articles to scrape\n",
    "        \n",
    "        Returns:\n",
    "            list: List of dictionaries containing basic article data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Construct the Google News search URL\n",
    "        base_url = \"https://news.google.com/search\"\n",
    "        params = {\n",
    "            'q': company_name,\n",
    "            'hl': 'en-IN',\n",
    "            'gl': 'IN',\n",
    "            'ceid': 'IN:en'\n",
    "        }\n",
    "        \n",
    "        search_url = f\"{base_url}?{urllib.parse.urlencode(params)}\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Searching for news articles about: {company_name}\")\n",
    "            print(f\"URL: {search_url}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Make the request\n",
    "            response = requests.get(search_url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all article containers\n",
    "            article_containers = soup.find_all('article') or soup.find_all(class_=\"xrnccd\")\n",
    "            \n",
    "            if not article_containers:\n",
    "                article_elements = soup.find_all(class_=\"WwrzSb\")\n",
    "                article_containers = []\n",
    "                for element in article_elements:\n",
    "                    container = element\n",
    "                    for _ in range(10):\n",
    "                        container = container.find_parent()\n",
    "                        if container and (container.name == 'article' or 'article' in str(container.get('class', []))):\n",
    "                            break\n",
    "                    if container:\n",
    "                        article_containers.append(container)\n",
    "            \n",
    "            if not article_containers:\n",
    "                print(\"No article containers found. The page structure might have changed.\")\n",
    "                return []\n",
    "            \n",
    "            # Extract basic data from articles\n",
    "            articles_data = []\n",
    "            for i, container in enumerate(article_containers[:max_articles]):\n",
    "                \n",
    "                article_data = {\n",
    "                    'google_news_url': 'URL not found',\n",
    "                    'date': 'Date not found',\n",
    "                    'author': 'Author not found',\n",
    "                    'source_title': 'Source not found',\n",
    "                    'source_image_url': 'Image not found',\n",
    "                    'content_image_url': 'Content image not found',\n",
    "                    'text_content': 'Content not found',\n",
    "                    'article_title': 'Title not found'\n",
    "                }\n",
    "                \n",
    "                # Extract URL\n",
    "                url_element = container.find(class_=\"WwrzSb\") or container.find(class_=\"JtKRv\")\n",
    "                if url_element:\n",
    "                    href = url_element.get('href')\n",
    "                    if href:\n",
    "                        if href.startswith('./'):\n",
    "                            article_data['google_news_url'] = f\"https://news.google.com{href[1:]}\"\n",
    "                        elif href.startswith('/'):\n",
    "                            article_data['google_news_url'] = f\"https://news.google.com{href}\"\n",
    "                        else:\n",
    "                            article_data['google_news_url'] = href\n",
    "                \n",
    "                # Extract source information\n",
    "                source_container = container.find(class_=\"oovtQ\")\n",
    "                if source_container:\n",
    "                    img_element = source_container.find('img')\n",
    "                    if img_element:\n",
    "                        img_src = img_element.get('src') or img_element.get('data-src')\n",
    "                        if img_src:\n",
    "                            article_data['source_image_url'] = img_src\n",
    "                    \n",
    "                    source_text = source_container.get_text(strip=True)\n",
    "                    if source_text:\n",
    "                        article_data['source_title'] = source_text\n",
    "                \n",
    "                # Extract content image\n",
    "                content_image_element = container.find(class_=\"Quavad vwBmvb\")\n",
    "                if content_image_element:\n",
    "                    img_tag = content_image_element.find('img')\n",
    "                    if img_tag:\n",
    "                        img_src = (img_tag.get('src') or \n",
    "                                  img_tag.get('data-src') or \n",
    "                                  img_tag.get('data-lazy-src') or\n",
    "                                  img_tag.get('srcset', '').split(',')[0].strip().split(' ')[0])\n",
    "                        \n",
    "                        if img_src:\n",
    "                            if img_src.startswith('http'):\n",
    "                                article_data['content_image_url'] = img_src\n",
    "                            elif img_src.startswith('//'):\n",
    "                                article_data['content_image_url'] = f\"https:{img_src}\"\n",
    "                            elif img_src.startswith('./'):\n",
    "                                article_data['content_image_url'] = f\"https://news.google.com{img_src[1:]}\"\n",
    "                            elif img_src.startswith('/'):\n",
    "                                article_data['content_image_url'] = f\"https://news.google.com{img_src}\"\n",
    "                \n",
    "                # Extract date and author\n",
    "                metadata_container = container.find(class_=\"UOVeFe\")\n",
    "                if metadata_container:\n",
    "                    date_element = metadata_container.find(class_=\"hvbAAd\")\n",
    "                    if date_element:\n",
    "                        article_data['date'] = date_element.get_text(strip=True)\n",
    "                    \n",
    "                    author_element = metadata_container.find(class_=\"bInasb\")\n",
    "                    if author_element:\n",
    "                        article_data['author'] = author_element.get_text(strip=True)\n",
    "                \n",
    "                # Extract article title\n",
    "                title_element = (container.find('h3') or \n",
    "                               container.find('h4') or \n",
    "                               container.find(class_=\"JtKRv\") or\n",
    "                               container.find(class_=\"mCBkyc\"))\n",
    "                if title_element:\n",
    "                    article_data['article_title'] = title_element.get_text(strip=True)\n",
    "                \n",
    "                # Extract text content/summary\n",
    "                content_selectors = [\n",
    "                    'div[class*=\"snippet\"]',\n",
    "                    'div[class*=\"summary\"]',\n",
    "                    'div[class*=\"description\"]',\n",
    "                    '.st',\n",
    "                    'span[class*=\"snippet\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_element = container.select_one(selector)\n",
    "                    if content_element:\n",
    "                        article_data['text_content'] = content_element.get_text(strip=True)\n",
    "                        break\n",
    "                \n",
    "                if article_data['text_content'] == 'Content not found':\n",
    "                    all_text = container.get_text(separator=' ', strip=True)\n",
    "                    content_parts = []\n",
    "                    for part in all_text.split():\n",
    "                        if len(' '.join(content_parts)) > 200:\n",
    "                            break\n",
    "                        content_parts.append(part)\n",
    "                    \n",
    "                    if content_parts:\n",
    "                        article_data['text_content'] = ' '.join(content_parts)\n",
    "                \n",
    "                articles_data.append(article_data)\n",
    "                \n",
    "                print(f\"Found Article {i+1}: {article_data['article_title']}\")\n",
    "            \n",
    "            return articles_data\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error making request: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_redirect_url(self, google_news_url):\n",
    "        \"\"\"Get the actual article URL from Google News redirect\"\"\"\n",
    "        try:\n",
    "            response = requests.get(google_news_url, headers=self.headers, allow_redirects=True, timeout=10)\n",
    "            return response.url\n",
    "        except:\n",
    "            return google_news_url\n",
    "\n",
    "    def get_news_summary_from_external(self, url):\n",
    "        \"\"\"\n",
    "        Call the get_news_summary function and return the result\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Calling get_news_summary for: {url}\")\n",
    "            \n",
    "            # Call the actual get_news_summary function\n",
    "            json_result = get_news_summary(url, return_json=False)\n",
    "            \n",
    "            return json_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling get_news_summary: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_detailed_article_data(self, url, include_full_content=True):\n",
    "        \"\"\"Extract comprehensive article data from URL using external summary function\"\"\"\n",
    "        try:\n",
    "            print(f\"Getting summary from external function for: {url}\")\n",
    "            \n",
    "            # Call the external summary function\n",
    "            summary_result = self.get_news_summary_from_external(url)\n",
    "            \n",
    "            if not summary_result or not summary_result.get('success'):\n",
    "                print(\"âœ— Failed to get summary from external function\")\n",
    "                print(summary_result)\n",
    "                return None\n",
    "            \n",
    "            # Extract data from the external function result\n",
    "            detailed_data = {\n",
    "                'final_url': summary_result.get('url', url),\n",
    "                'scraped_at': summary_result.get('timestamp', datetime.now().isoformat()),\n",
    "                'detailed_title': summary_result.get('article', {}).get('title', 'Title not found'),\n",
    "                'detailed_author': summary_result.get('article', {}).get('authors', ['Author not found']),\n",
    "                'detailed_publish_date': summary_result.get('article', {}).get('publish_date', 'Date not found'),\n",
    "                'word_count': summary_result.get('article', {}).get('word_count', 0),\n",
    "                'main_image_url': summary_result.get('article', {}).get('top_image', 'Image not found'),\n",
    "                'content_summary': summary_result.get('summaries', {}).get('newspaper3k', 'Summary not available'),\n",
    "                'textblob_summary': summary_result.get('summaries', {}).get('textblob', 'TextBlob summary not available'),\n",
    "                'sentiment_analysis': summary_result.get('sentiment_analysis', {}),\n",
    "                'keywords': summary_result.get('keywords', []),\n",
    "                'external_summary_success': True\n",
    "            }\n",
    "            \n",
    "            # Add full content flag but don't include actual full content since we're using external summary\n",
    "            if include_full_content:\n",
    "                detailed_data['full_content_note'] = \"Full content extraction skipped - using external summary function\"\n",
    "            \n",
    "            return detailed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting detailed article data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_comprehensive_news(self, company_name, max_articles=3, extract_full_content=True):\n",
    "        \"\"\"\n",
    "        Main method to scrape comprehensive news data using external summary function\n",
    "        \n",
    "        Args:\n",
    "            company_name (str): Company name to search for\n",
    "            max_articles (int): Maximum number of articles to process\n",
    "            extract_full_content (bool): Flag for compatibility (not used with external summary)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Comprehensive news data\n",
    "        \"\"\"\n",
    "        print(f\"Starting comprehensive news scraping for: {company_name}\")\n",
    "        print(f\"Using external get_news_summary function\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Step 1: Get basic articles from Google News\n",
    "        basic_articles = self.scrape_google_news_articles(company_name, max_articles)\n",
    "        \n",
    "        if not basic_articles:\n",
    "            return {\n",
    "                'search_query': company_name,\n",
    "                'scraped_at': datetime.now().isoformat(),\n",
    "                'total_articles_found': 0,\n",
    "                'articles': []\n",
    "            }\n",
    "        \n",
    "        # Step 2: Extract detailed content for each article using external function\n",
    "        comprehensive_articles = []\n",
    "        \n",
    "        for i, basic_article in enumerate(basic_articles):\n",
    "            print(f\"\\nProcessing article {i+1}/{len(basic_articles)}\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            # Merge basic data\n",
    "            comprehensive_article = {\n",
    "                'article_id': i + 1,\n",
    "                'google_news_data': basic_article,\n",
    "                'detailed_data': None,\n",
    "                'extraction_success': False\n",
    "            }\n",
    "            \n",
    "            if basic_article['google_news_url'] != 'URL not found':\n",
    "                # Get the actual article URL\n",
    "                try:\n",
    "                    actual_url = self.get_redirect_url(basic_article['google_news_url'])\n",
    "                    print(f\"Actual URL: {actual_url}\")\n",
    "                    \n",
    "                    # Extract detailed content using external summary function\n",
    "                    detailed_data = self.extract_detailed_article_data(actual_url, extract_full_content)\n",
    "                    \n",
    "                    if detailed_data:\n",
    "                        comprehensive_article['detailed_data'] = detailed_data\n",
    "                        comprehensive_article['extraction_success'] = True\n",
    "                        print(\"âœ“ Successfully extracted summary using external function\")\n",
    "                        \n",
    "                        # Print the newspaper3k summary\n",
    "                        newspaper_summary = detailed_data.get('content_summary', 'No summary available')\n",
    "                        print(f\"âœ“ Newspaper3k Summary: {newspaper_summary[:100]}...\")\n",
    "                    else:\n",
    "                        print(\"âœ— Failed to extract content using external function\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Error processing article: {str(e)}\")\n",
    "            \n",
    "            comprehensive_articles.append(comprehensive_article)\n",
    "            \n",
    "            # Add delay between requests\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Compile final result\n",
    "        result = {\n",
    "            'search_query': company_name,\n",
    "            'scraped_at': datetime.now().isoformat(),\n",
    "            'total_articles_found': len(comprehensive_articles),\n",
    "            'successful_extractions': sum(1 for article in comprehensive_articles if article['extraction_success']),\n",
    "            'summary_method': 'external_get_news_summary_function',\n",
    "            'articles': comprehensive_articles\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def save_comprehensive_data(self, data, filename=None):\n",
    "        \"\"\"Save comprehensive data to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"comprehensive_news_external_summary_{data['search_query'].replace(' ', '_')}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nComprehensive news data saved to: {filename}\")\n",
    "        return filename\n",
    "\n",
    "    def print_summary(self, data):\n",
    "        \"\"\"Print a summary of the scraped data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"COMPREHENSIVE NEWS SCRAPING SUMMARY (Using External Summary Function)\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Search Query: {data['search_query']}\")\n",
    "        print(f\"Scraped At: {data['scraped_at']}\")\n",
    "        print(f\"Total Articles Found: {data['total_articles_found']}\")\n",
    "        print(f\"Successful External Extractions: {data['successful_extractions']}\")\n",
    "        print(f\"Summary Method: {data.get('summary_method', 'external_function')}\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        for article in data['articles']:\n",
    "            print(f\"\\nArticle {article['article_id']}:\")\n",
    "            print(f\"  Title: {article['google_news_data']['article_title']}\")\n",
    "            print(f\"  Source: {article['google_news_data']['source_title']}\")\n",
    "            print(f\"  Date: {article['google_news_data']['date']}\")\n",
    "            print(f\"  External Summary Extraction: {'âœ“' if article['extraction_success'] else 'âœ—'}\")\n",
    "            \n",
    "            if article['detailed_data']:\n",
    "                # Print newspaper3k summary\n",
    "                newspaper_summary = article['detailed_data'].get('content_summary', 'No summary available')\n",
    "                if newspaper_summary != 'No summary available':\n",
    "                    summary_preview = newspaper_summary[:150]\n",
    "                    print(f\"  Newspaper3k Summary: {summary_preview}...\")\n",
    "                \n",
    "                # Print additional extracted data\n",
    "                word_count = article['detailed_data'].get('word_count', 0)\n",
    "                if word_count > 0:\n",
    "                    print(f\"  Word Count: {word_count}\")\n",
    "                \n",
    "                keywords = article['detailed_data'].get('keywords', [])\n",
    "                if keywords:\n",
    "                    print(f\"  Keywords: {', '.join(keywords[:5])}\")\n",
    "                \n",
    "                sentiment = article['detailed_data'].get('sentiment_analysis', {})\n",
    "                if sentiment:\n",
    "                    sentiment_label = sentiment.get('sentiment_label', 'unknown')\n",
    "                    polarity = sentiment.get('polarity', 0)\n",
    "                    print(f\"  Sentiment: {sentiment_label} (polarity: {polarity})\")\n",
    "        \n",
    "        print(\"=\"*100)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the comprehensive news scraper with external summary\"\"\"\n",
    "    print(\"Enhanced Comprehensive News Scraper (Using External Summary Function)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get user input\n",
    "    company_name = input(\"Enter the company name to search for: \").strip()\n",
    "    \n",
    "    if not company_name:\n",
    "        print(\"Please enter a valid company name.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        max_articles = int(input(\"Enter max number of articles to process (default: 3): \").strip() or \"3\")\n",
    "    except ValueError:\n",
    "        max_articles = 3\n",
    "    \n",
    "    print(\"Note: Using external get_news_summary function for content extraction and summarization\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = ComprehensiveNewsScraper(headless=True)\n",
    "    \n",
    "    # Scrape comprehensive news data\n",
    "    print(f\"\\nStarting comprehensive scraping...\")\n",
    "    comprehensive_data = scraper.scrape_comprehensive_news(\n",
    "        company_name, \n",
    "        max_articles=max_articles,\n",
    "        extract_full_content=True  # This flag is maintained for compatibility but not used\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    scraper.print_summary(comprehensive_data)\n",
    "    \n",
    "    # Save to file\n",
    "    save_choice = input(\"\\nSave results to JSON file? (y/n): \").strip().lower()\n",
    "    if save_choice == 'y':\n",
    "        filename = input(\"Enter filename (press Enter for default): \").strip()\n",
    "        if not filename:\n",
    "            filename = None\n",
    "        \n",
    "        saved_file = scraper.save_comprehensive_data(comprehensive_data, filename)\n",
    "        print(f\"Data saved to: {saved_file}\")\n",
    "    \n",
    "    print(\"\\nScraping completed using external summary function!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec6b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Comprehensive News Scraper\n",
      "==================================================\n",
      "Note: Will extract summary, meta description, and all metadata but exclude full content from JSON\n",
      "\n",
      "Starting comprehensive scraping...\n",
      "Starting comprehensive news scraping for: swiggy\n",
      "Extract full content: False\n",
      "================================================================================\n",
      "Searching for news articles about: swiggy\n",
      "URL: https://news.google.com/search?q=swiggy&hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "--------------------------------------------------\n",
      "Found Article 1: 'Not out of need, but to rebuild ties': Bengaluru techie uses Swiggy deliveries to meet people, find clients\n",
      "Found Article 2: MSCI rejig: Swiggy, Mazagon Dock, two others among likely additions to India Standard Index in August rebalancing\n",
      "Found Article 3: MSCI rejig: Swiggy, Mazagon Dock among 4 entrants; $850 mn inflows likely\n",
      "\n",
      "Processing article 1/3\n",
      "--------------------------------------------------\n",
      "Actual URL: https://www.google.com/sorry/index?continue=https://news.google.com/read/CBMijAJBVV95cUxNTnBWX19WVVUwSDBOQm10enpoMi1OUmpReXBGR2FtbGtmYkQxX2F1TkVwcl9iWXVuN3YyOS1ZRU05VTlKZTRVLUFaZFN5b0VFR19pV1E5VU80RFU4WENHeXE4Um5HdHRiSGd6NE5UVUtpd05wbEN6aG52YUZBdnQ1RFlKSkRyRWR4YVNkUTMzRW85bUZsNW5VVHFvYWd2WWtaOGVnRFMxQ0ZkQUxwMWs5WHRGdUZoTm41OGRRbFNwRk02SEhwbkh1ZklrSXRHbGVxZXprbWloVXhZeDh0dzR5a0NTYmtjdkpfa0M2ZlRaYUpmeXZzOU1OUkxyMWo1WFJrN0E4MXdHZ29KODNm%3Fhl%3Den-IN%26gl%3DIN%26ceid%3DIN%253Aen&hl=en-IN&q=EhAkAUkAHMiGkBFDxVN9L0kaGNz1vsIGIjAaw9p1mKm1fc8gbaTbQ0kiVLYcXH_fRZ0DjsqXoQdxjsSNgIeJicLbwHFDEXG06IgyAnJSWgFD\n",
      "Extracting detailed content from: https://www.google.com/sorry/index?continue=https://news.google.com/read/CBMijAJBVV95cUxNTnBWX19WVVUwSDBOQm10enpoMi1OUmpReXBGR2FtbGtmYkQxX2F1TkVwcl9iWXVuN3YyOS1ZRU05VTlKZTRVLUFaZFN5b0VFR19pV1E5VU80RFU4WENHeXE4Um5HdHRiSGd6NE5UVUtpd05wbEN6aG52YUZBdnQ1RFlKSkRyRWR4YVNkUTMzRW85bUZsNW5VVHFvYWd2WWtaOGVnRFMxQ0ZkQUxwMWs5WHRGdUZoTm41OGRRbFNwRk02SEhwbkh1ZklrSXRHbGVxZXprbWloVXhZeDh0dzR5a0NTYmtjdkpfa0M2ZlRaYUpmeXZzOU1OUkxyMWo1WFJrN0E4MXdHZ29KODNm%3Fhl%3Den-IN%26gl%3DIN%26ceid%3DIN%253Aen&hl=en-IN&q=EhAkAUkAHMiGkBFDxVN9L0kaGNz1vsIGIjAaw9p1mKm1fc8gbaTbQ0kiVLYcXH_fRZ0DjsqXoQdxjsSNgIeJicLbwHFDEXG06IgyAnJSWgFD\n",
      "âœ“ Successfully extracted detailed content\n",
      "âœ“ Summary and metadata extracted (full content excluded)\n",
      "\n",
      "Processing article 2/3\n",
      "--------------------------------------------------\n",
      "Actual URL: https://www.google.com/sorry/index?continue=https://news.google.com/read/CBMijAJBVV95cUxPOWVFMURLZ2RGY0N1Wk5sT0tTS3NhcHJEYUJOYW1abEtpVzJTZG1pcUFpUlBaNWVEQ3UxQlh2QURJX01ld0ZEbWFSeEFtOU1Sb0Y0NV9XaW54UnY1OFV1Y2RWdmZ6ZkRyMVlWTEo5eWh2N0F4d1NRZlo1UnVvMmZzdzdYY2t0OHRLcGxkaHB4LWJZVHBTTThQRGJMQnA4MEZMRFNLdjlrZmdHYlNwSm1aeDViWVFlXzNPZ1g1Xy04UlV2NnZvOGczb2x5U2tFeGNMekpBNXl1V3lBNE1GbTgyUDZkV0tFUmNBNUVMWWhtYVNvdTEwNHhfcGFUVGxKQ0hYTXFSWjhyOGx2TEhQ0gGSAkFVX3lxTE9oQXN4UHczTngzNTEwUmhwanNmMjkzSG9WVzRNOFR0cktkYVVSYjg3ZUJfX2xobldUWFVKdXFGbWhYRnJjNWRCMDlMWmdsc19FSzBEQlY1eENFdElKN241b1Q1a3VmVTNJb2IyYzJvd1pyN2JIdXl0MEctMldsT1p3MXFNQ0YyUlRScWdmWTNkR2I4dUxadUxhZ2hKQUZmLVFHZTR4TWZIaXRKNUw5OXB0cmRpdzdaUDdhaDZHMjJRdkRDS2hmTTNmNzhmWXhfZHJmcTdtVTM5NnQwMG5Na2kzME5EajJZX2tOMlNqcjhOekdsX2xVZU1KSWNDVlU4aWJOa05TeGR2akU0UVJ2WWNTTWc%3Fhl%3Den-IN%26gl%3DIN%26ceid%3DIN%253Aen&hl=en-IN&q=EhAkAUkAHMiGkBFDxVN9L0kaGOj1vsIGIjDOZcBBEjcEUxNRlfRywVn7Gp-hfpzqsm2Bg-rn-zwulv3oSEm7lDlUvNQViEv3WaYyAnJSWgFD\n",
      "Extracting detailed content from: https://www.google.com/sorry/index?continue=https://news.google.com/read/CBMijAJBVV95cUxPOWVFMURLZ2RGY0N1Wk5sT0tTS3NhcHJEYUJOYW1abEtpVzJTZG1pcUFpUlBaNWVEQ3UxQlh2QURJX01ld0ZEbWFSeEFtOU1Sb0Y0NV9XaW54UnY1OFV1Y2RWdmZ6ZkRyMVlWTEo5eWh2N0F4d1NRZlo1UnVvMmZzdzdYY2t0OHRLcGxkaHB4LWJZVHBTTThQRGJMQnA4MEZMRFNLdjlrZmdHYlNwSm1aeDViWVFlXzNPZ1g1Xy04UlV2NnZvOGczb2x5U2tFeGNMekpBNXl1V3lBNE1GbTgyUDZkV0tFUmNBNUVMWWhtYVNvdTEwNHhfcGFUVGxKQ0hYTXFSWjhyOGx2TEhQ0gGSAkFVX3lxTE9oQXN4UHczTngzNTEwUmhwanNmMjkzSG9WVzRNOFR0cktkYVVSYjg3ZUJfX2xobldUWFVKdXFGbWhYRnJjNWRCMDlMWmdsc19FSzBEQlY1eENFdElKN241b1Q1a3VmVTNJb2IyYzJvd1pyN2JIdXl0MEctMldsT1p3MXFNQ0YyUlRScWdmWTNkR2I4dUxadUxhZ2hKQUZmLVFHZTR4TWZIaXRKNUw5OXB0cmRpdzdaUDdhaDZHMjJRdkRDS2hmTTNmNzhmWXhfZHJmcTdtVTM5NnQwMG5Na2kzME5EajJZX2tOMlNqcjhOekdsX2xVZU1KSWNDVlU4aWJOa05TeGR2akU0UVJ2WWNTTWc%3Fhl%3Den-IN%26gl%3DIN%26ceid%3DIN%253Aen&hl=en-IN&q=EhAkAUkAHMiGkBFDxVN9L0kaGOj1vsIGIjDOZcBBEjcEUxNRlfRywVn7Gp-hfpzqsm2Bg-rn-zwulv3oSEm7lDlUvNQViEv3WaYyAnJSWgFD\n",
      "âœ“ Successfully extracted detailed content\n",
      "âœ“ Summary and metadata extracted (full content excluded)\n",
      "\n",
      "Processing article 3/3\n",
      "--------------------------------------------------\n",
      "Actual URL: https://www.google.com/sorry/index?continue=https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0%3Fhl%3Den-IN%26gl%3DIN%26ceid%3DIN%253Aen&hl=en-IN&q=EhAkAUkAHMiGkBFDxVN9L0kaGPP1vsIGIjCOKW2vrJq23FIjme5eWz9wj3ePfoJxrYl4rzngmrxz0w-jR2BtMTeEZr8-EWrADH4yAnJSWgFD\n",
      "Extracting detailed content from: https://www.google.com/sorry/index?continue=https://news.google.com/read/CBMi0gFBVV95cUxPZ21qcVdFamVObDZmbUZNQ1c2VXExazN4blNYQUh4WGUzX3ZSNGMwUDcxVlRXeDlQMnIyeEx2ZVVicGJmeE5IWEFCSFNpTFUtZDFXR1FRWVBGWFhHMm5hYjRSUXp1MzNfNUFPVWRaZjZaVTYyc2ZwMmcxcjd5TXlWWlpoNUF6RTQzaE9jcklLS0pacVhvWGFDNXRPS0I3YTZpMDNoRzdvQXhPdTl3RHRxN3Rfa0s4M0NGU3lKNVJDaG5QSjlxRHVtVmNqdFk5VUN6MGfSAdcBQVVfeXFMUGlVMnk2dm9lWklWYW0yMUpkbzZiakpSZmI4TjlqanVzMGR3Yl80QnhibjFLSWpjV3B4cm04VXpTMllWcDZBeTdNOUUzaWRjVzZTWXBaQ0FReGdxNkhBTElXb2VfWDU1VEhsSGpQQlJucmhsclpfR2IwR194bXQxSTdDM3EtYUo2MXlobTNpRTFubjJUSXlUVy1sbzJQVXVZMUZGT2p0Z1FwN2tjb3ZIMEFLSVZiYVhsMWdYQUZuT092VnJOSjFFY0dnT2tBVWc2dk5PT2Fpck0%3Fhl%3Den-IN%26gl%3DIN%26ceid%3DIN%253Aen&hl=en-IN&q=EhAkAUkAHMiGkBFDxVN9L0kaGPP1vsIGIjCOKW2vrJq23FIjme5eWz9wj3ePfoJxrYl4rzngmrxz0w-jR2BtMTeEZr8-EWrADH4yAnJSWgFD\n",
      "âœ“ Successfully extracted detailed content\n",
      "âœ“ Summary and metadata extracted (full content excluded)\n",
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE NEWS SCRAPING SUMMARY\n",
      "====================================================================================================\n",
      "Search Query: swiggy\n",
      "Scraped At: 2025-06-16T12:04:36.985479\n",
      "Total Articles Found: 3\n",
      "Successful Detailed Extractions: 3\n",
      "Full Content Included: No (Summary only)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Article 1:\n",
      "  Title: 'Not out of need, but to rebuild ties': Bengaluru techie uses Swiggy deliveries to meet people, find clients\n",
      "  Source: Times of India\n",
      "  Date: 3 hours ago\n",
      "  Detailed Extraction: âœ“\n",
      "  Full Content: Not included (summary-only mode)\n",
      "  Summary: Content too short for summary...\n",
      "\n",
      "Article 2:\n",
      "  Title: MSCI rejig: Swiggy, Mazagon Dock, two others among likely additions to India Standard Index in August rebalancing\n",
      "  Source: Mint\n",
      "  Date: 2 days ago\n",
      "  Detailed Extraction: âœ“\n",
      "  Full Content: Not included (summary-only mode)\n",
      "  Summary: Content too short for summary...\n",
      "\n",
      "Article 3:\n",
      "  Title: MSCI rejig: Swiggy, Mazagon Dock among 4 entrants; $850 mn inflows likely\n",
      "  Source: Business Standard\n",
      "  Date: 54 minutes ago\n",
      "  Detailed Extraction: âœ“\n",
      "  Full Content: Not included (summary-only mode)\n",
      "  Summary: Content too short for summary...\n",
      "====================================================================================================\n",
      "\n",
      "Comprehensive news data saved to: swiggyoriginal.json\n",
      "Data saved to: swiggyoriginal.json\n",
      "\n",
      "Scraping completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "class ComprehensiveNewsScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        \"\"\"Initialize the comprehensive news scraper\"\"\"\n",
    "        self.chrome_options = Options()\n",
    "        if headless:\n",
    "            self.chrome_options.add_argument(\"--headless\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "        \n",
    "        # Headers for requests\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "\n",
    "    def scrape_google_news_articles(self, company_name, max_articles=5):\n",
    "        \"\"\"\n",
    "        Scrapes Google News for articles about a specific company\n",
    "        \n",
    "        Args:\n",
    "            company_name (str): Name of the company to search for\n",
    "            max_articles (int): Maximum number of articles to scrape\n",
    "        \n",
    "        Returns:\n",
    "            list: List of dictionaries containing basic article data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Construct the Google News search URL\n",
    "        base_url = \"https://news.google.com/search\"\n",
    "        params = {\n",
    "            'q': company_name,\n",
    "            'hl': 'en-IN',\n",
    "            'gl': 'IN',\n",
    "            'ceid': 'IN:en'\n",
    "        }\n",
    "        \n",
    "        search_url = f\"{base_url}?{urllib.parse.urlencode(params)}\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Searching for news articles about: {company_name}\")\n",
    "            print(f\"URL: {search_url}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Make the request\n",
    "            response = requests.get(search_url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all article containers\n",
    "            article_containers = soup.find_all('article') or soup.find_all(class_=\"xrnccd\")\n",
    "            \n",
    "            if not article_containers:\n",
    "                article_elements = soup.find_all(class_=\"WwrzSb\")\n",
    "                article_containers = []\n",
    "                for element in article_elements:\n",
    "                    container = element\n",
    "                    for _ in range(10):\n",
    "                        container = container.find_parent()\n",
    "                        if container and (container.name == 'article' or 'article' in str(container.get('class', []))):\n",
    "                            break\n",
    "                    if container:\n",
    "                        article_containers.append(container)\n",
    "            \n",
    "            if not article_containers:\n",
    "                print(\"No article containers found. The page structure might have changed.\")\n",
    "                return []\n",
    "            \n",
    "            # Extract basic data from articles\n",
    "            articles_data = []\n",
    "            for i, container in enumerate(article_containers[:max_articles]):\n",
    "                \n",
    "                article_data = {\n",
    "                    'google_news_url': 'URL not found',\n",
    "                    'date': 'Date not found',\n",
    "                    'author': 'Author not found',\n",
    "                    'source_title': 'Source not found',\n",
    "                    'source_image_url': 'Image not found',\n",
    "                    'content_image_url': 'Content image not found',\n",
    "                    'text_content': 'Content not found',\n",
    "                    'article_title': 'Title not found'\n",
    "                }\n",
    "                \n",
    "                # Extract URL\n",
    "                url_element = container.find(class_=\"WwrzSb\") or container.find(class_=\"JtKRv\")\n",
    "                if url_element:\n",
    "                    href = url_element.get('href')\n",
    "                    if href:\n",
    "                        if href.startswith('./'):\n",
    "                            article_data['google_news_url'] = f\"https://news.google.com{href[1:]}\"\n",
    "                        elif href.startswith('/'):\n",
    "                            article_data['google_news_url'] = f\"https://news.google.com{href}\"\n",
    "                        else:\n",
    "                            article_data['google_news_url'] = href\n",
    "                \n",
    "                # Extract source information\n",
    "                source_container = container.find(class_=\"oovtQ\")\n",
    "                if source_container:\n",
    "                    img_element = source_container.find('img')\n",
    "                    if img_element:\n",
    "                        img_src = img_element.get('src') or img_element.get('data-src')\n",
    "                        if img_src:\n",
    "                            article_data['source_image_url'] = img_src\n",
    "                    \n",
    "                    source_text = source_container.get_text(strip=True)\n",
    "                    if source_text:\n",
    "                        article_data['source_title'] = source_text\n",
    "                \n",
    "                # Extract content image\n",
    "                content_image_element = container.find(class_=\"Quavad vwBmvb\")\n",
    "                if content_image_element:\n",
    "                    img_tag = content_image_element.find('img')\n",
    "                    if img_tag:\n",
    "                        img_src = (img_tag.get('src') or \n",
    "                                  img_tag.get('data-src') or \n",
    "                                  img_tag.get('data-lazy-src') or\n",
    "                                  img_tag.get('srcset', '').split(',')[0].strip().split(' ')[0])\n",
    "                        \n",
    "                        if img_src:\n",
    "                            if img_src.startswith('http'):\n",
    "                                article_data['content_image_url'] = img_src\n",
    "                            elif img_src.startswith('//'):\n",
    "                                article_data['content_image_url'] = f\"https:{img_src}\"\n",
    "                            elif img_src.startswith('./'):\n",
    "                                article_data['content_image_url'] = f\"https://news.google.com{img_src[1:]}\"\n",
    "                            elif img_src.startswith('/'):\n",
    "                                article_data['content_image_url'] = f\"https://news.google.com{img_src}\"\n",
    "                \n",
    "                # Extract date and author\n",
    "                metadata_container = container.find(class_=\"UOVeFe\")\n",
    "                if metadata_container:\n",
    "                    date_element = metadata_container.find(class_=\"hvbAAd\")\n",
    "                    if date_element:\n",
    "                        article_data['date'] = date_element.get_text(strip=True)\n",
    "                    \n",
    "                    author_element = metadata_container.find(class_=\"bInasb\")\n",
    "                    if author_element:\n",
    "                        article_data['author'] = author_element.get_text(strip=True)\n",
    "                \n",
    "                # Extract article title\n",
    "                title_element = (container.find('h3') or \n",
    "                               container.find('h4') or \n",
    "                               container.find(class_=\"JtKRv\") or\n",
    "                               container.find(class_=\"mCBkyc\"))\n",
    "                if title_element:\n",
    "                    article_data['article_title'] = title_element.get_text(strip=True)\n",
    "                \n",
    "                # Extract text content/summary\n",
    "                content_selectors = [\n",
    "                    'div[class*=\"snippet\"]',\n",
    "                    'div[class*=\"summary\"]',\n",
    "                    'div[class*=\"description\"]',\n",
    "                    '.st',\n",
    "                    'span[class*=\"snippet\"]'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_element = container.select_one(selector)\n",
    "                    if content_element:\n",
    "                        article_data['text_content'] = content_element.get_text(strip=True)\n",
    "                        break\n",
    "                \n",
    "                if article_data['text_content'] == 'Content not found':\n",
    "                    all_text = container.get_text(separator=' ', strip=True)\n",
    "                    content_parts = []\n",
    "                    for part in all_text.split():\n",
    "                        if len(' '.join(content_parts)) > 200:\n",
    "                            break\n",
    "                        content_parts.append(part)\n",
    "                    \n",
    "                    if content_parts:\n",
    "                        article_data['text_content'] = ' '.join(content_parts)\n",
    "                \n",
    "                articles_data.append(article_data)\n",
    "                \n",
    "                print(f\"Found Article {i+1}: {article_data['article_title']}\")\n",
    "            \n",
    "            return articles_data\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error making request: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_redirect_url(self, google_news_url):\n",
    "        \"\"\"Get the actual article URL from Google News redirect\"\"\"\n",
    "        try:\n",
    "            response = requests.get(google_news_url, headers=self.headers, allow_redirects=True, timeout=10)\n",
    "            return response.url\n",
    "        except:\n",
    "            return google_news_url\n",
    "\n",
    "    def extract_detailed_article_data(self, url, include_full_content=True):\n",
    "        \"\"\"Extract comprehensive article data from URL using Selenium\"\"\"\n",
    "        driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Extracting detailed content from: {url}\")\n",
    "            \n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "            )\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Get final URL after redirects\n",
    "            final_url = driver.current_url\n",
    "            \n",
    "            # Get the raw HTML\n",
    "            raw_html = driver.page_source\n",
    "            \n",
    "            # Parse the page\n",
    "            soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "            \n",
    "            # Extract comprehensive article data\n",
    "            detailed_data = self._extract_all_detailed_data(soup, final_url, include_full_content)\n",
    "            \n",
    "            return detailed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting detailed article data: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    def _extract_all_detailed_data(self, soup, url, include_full_content=True):\n",
    "        \"\"\"Extract all available detailed article data\"\"\"\n",
    "        # Always extract the content first for summary generation\n",
    "        full_content = self._extract_content(soup)\n",
    "        \n",
    "        data = {\n",
    "            'final_url': url,\n",
    "            'scraped_at': datetime.now().isoformat(),\n",
    "            'detailed_title': self._extract_title(soup),\n",
    "            'meta_description': self._extract_meta_description(soup),\n",
    "            'detailed_author': self._extract_author(soup),\n",
    "            'detailed_publish_date': self._extract_publish_date(soup),\n",
    "            'keywords': self._extract_keywords(soup),\n",
    "            'category': self._extract_category(soup),\n",
    "            'detailed_source': self._extract_source(soup),\n",
    "            'main_image_url': self._extract_main_image(soup),\n",
    "            'content_summary': None\n",
    "        }\n",
    "        \n",
    "        # Always generate summary from content\n",
    "        if full_content:\n",
    "            data['content_summary'] = self._generate_summary(full_content)\n",
    "        \n",
    "        # Only include full content if requested\n",
    "        if include_full_content:\n",
    "            data['full_content'] = full_content\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def _extract_title(self, soup):\n",
    "        \"\"\"Extract article title\"\"\"\n",
    "        selectors = [\n",
    "            'h1',\n",
    "            '[data-testid=\"headline\"]',\n",
    "            '.article-title',\n",
    "            '.entry-title',\n",
    "            'meta[property=\"og:title\"]',\n",
    "            'title'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element and element.get_text().strip():\n",
    "                    return element.get_text().strip()\n",
    "        \n",
    "        return \"Title not found\"\n",
    "\n",
    "    def _extract_meta_description(self, soup):\n",
    "        \"\"\"Extract meta description\"\"\"\n",
    "        meta_selectors = [\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"twitter:description\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in meta_selectors:\n",
    "            meta = soup.select_one(selector)\n",
    "            if meta and meta.get('content'):\n",
    "                return meta.get('content').strip()\n",
    "        \n",
    "        return \"Meta description not found\"\n",
    "\n",
    "    def _extract_author(self, soup):\n",
    "        \"\"\"Extract article author\"\"\"\n",
    "        author_selectors = [\n",
    "            'meta[name=\"author\"]',\n",
    "            'meta[property=\"article:author\"]',\n",
    "            '.author',\n",
    "            '.byline',\n",
    "            '[rel=\"author\"]',\n",
    "            '.article-author'\n",
    "        ]\n",
    "        \n",
    "        for selector in author_selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get_text().strip()\n",
    "        \n",
    "        return \"Author not found\"\n",
    "\n",
    "    def _extract_publish_date(self, soup):\n",
    "        \"\"\"Extract publish date\"\"\"\n",
    "        date_selectors = [\n",
    "            'meta[property=\"article:published_time\"]',\n",
    "            'meta[name=\"publish_date\"]',\n",
    "            'time[datetime]',\n",
    "            '.publish-date',\n",
    "            '.article-date'\n",
    "        ]\n",
    "        \n",
    "        for selector in date_selectors:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                if selector.startswith('meta'):\n",
    "                    return element.get('content', '').strip()\n",
    "                elif element.name == 'time':\n",
    "                    return element.get('datetime', element.get_text()).strip()\n",
    "                else:\n",
    "                    return element.get_text().strip()\n",
    "        \n",
    "        return \"Publish date not found\"\n",
    "\n",
    "    def _extract_content(self, soup):\n",
    "        \"\"\"Extract main article content\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
    "            element.decompose()\n",
    "        \n",
    "        content_selectors = [\n",
    "            'article',\n",
    "            '.article-content',\n",
    "            '.entry-content',\n",
    "            '.post-content',\n",
    "            '.content',\n",
    "            'main',\n",
    "            '[data-testid=\"article-content\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in content_selectors:\n",
    "            content_div = soup.select_one(selector)\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all(['p', 'div'], class_=lambda x: x != 'advertisement' if x else True)\n",
    "                content_text = []\n",
    "                \n",
    "                for p in paragraphs:\n",
    "                    text = p.get_text().strip()\n",
    "                    if len(text) > 50:\n",
    "                        content_text.append(text)\n",
    "                \n",
    "                if content_text:\n",
    "                    return '\\n\\n'.join(content_text)\n",
    "        \n",
    "        # Fallback: get all paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text().strip()\n",
    "            if len(text) > 50:\n",
    "                content.append(text)\n",
    "        \n",
    "        return '\\n\\n'.join(content) if content else \"Content not found\"\n",
    "\n",
    "    def _extract_keywords(self, soup):\n",
    "        \"\"\"Extract keywords/tags\"\"\"\n",
    "        keyword_selectors = [\n",
    "            'meta[name=\"keywords\"]',\n",
    "            'meta[property=\"article:tag\"]',\n",
    "            '.tags',\n",
    "            '.keywords'\n",
    "        ]\n",
    "        \n",
    "        for selector in keyword_selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                elements = soup.select(selector + ' a, ' + selector + ' span')\n",
    "                if elements:\n",
    "                    return ', '.join([el.get_text().strip() for el in elements])\n",
    "        \n",
    "        return \"Keywords not found\"\n",
    "\n",
    "    def _extract_category(self, soup):\n",
    "        \"\"\"Extract article category\"\"\"\n",
    "        category_selectors = [\n",
    "            'meta[property=\"article:section\"]',\n",
    "            '.category',\n",
    "            '.section',\n",
    "            '[data-category]'\n",
    "        ]\n",
    "        \n",
    "        for selector in category_selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            elif selector == '[data-category]':\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('data-category', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get_text().strip()\n",
    "        \n",
    "        return \"Category not found\"\n",
    "\n",
    "    def _extract_source(self, soup):\n",
    "        \"\"\"Extract source/publisher\"\"\"\n",
    "        source_selectors = [\n",
    "            'meta[property=\"og:site_name\"]',\n",
    "            'meta[name=\"publisher\"]',\n",
    "            '.source',\n",
    "            '.publisher'\n",
    "        ]\n",
    "        \n",
    "        for selector in source_selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get_text().strip()\n",
    "        \n",
    "        return \"Source not found\"\n",
    "\n",
    "    def _extract_main_image(self, soup):\n",
    "        \"\"\"Extract main article image\"\"\"\n",
    "        image_selectors = [\n",
    "            'meta[property=\"og:image\"]',\n",
    "            'meta[name=\"twitter:image\"]',\n",
    "            'article img',\n",
    "            '.article-image img',\n",
    "            '.featured-image img'\n",
    "        ]\n",
    "        \n",
    "        for selector in image_selectors:\n",
    "            if selector.startswith('meta'):\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('content', '').strip()\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get('src', '').strip()\n",
    "        \n",
    "        return \"Main image not found\"\n",
    "\n",
    "    def _generate_summary(self, content, max_sentences=3):\n",
    "        \"\"\"Generate a simple extractive summary\"\"\"\n",
    "        if not content or len(content) < 100:\n",
    "            return \"Content too short for summary\"\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'[.!?]+', content)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "        \n",
    "        if len(sentences) <= max_sentences:\n",
    "            return '. '.join(sentences[:max_sentences]) + '.'\n",
    "        \n",
    "        # Simple scoring: prefer sentences with more words\n",
    "        scored_sentences = []\n",
    "        for i, sentence in enumerate(sentences[:10]):\n",
    "            score = len(sentence.split())\n",
    "            if any(word in sentence.lower() for word in ['said', 'according', 'reported', 'announced']):\n",
    "                score += 5\n",
    "            scored_sentences.append((score, sentence, i))\n",
    "        \n",
    "        # Sort by score and take top sentences, maintaining original order\n",
    "        top_sentences = sorted(scored_sentences, key=lambda x: x[0], reverse=True)[:max_sentences]\n",
    "        top_sentences = sorted(top_sentences, key=lambda x: x[2])\n",
    "        \n",
    "        summary = '. '.join([s[1] for s in top_sentences])\n",
    "        return summary + '.' if not summary.endswith('.') else summary\n",
    "\n",
    "    def scrape_comprehensive_news(self, company_name, max_articles=3, extract_full_content=True):\n",
    "        \"\"\"\n",
    "        Main method to scrape comprehensive news data\n",
    "        \n",
    "        Args:\n",
    "            company_name (str): Company name to search for\n",
    "            max_articles (int): Maximum number of articles to process\n",
    "            extract_full_content (bool): Whether to extract and include full article content in JSON\n",
    "        \n",
    "        Returns:\n",
    "            dict: Comprehensive news data\n",
    "        \"\"\"\n",
    "        print(f\"Starting comprehensive news scraping for: {company_name}\")\n",
    "        print(f\"Extract full content: {extract_full_content}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Step 1: Get basic articles from Google News\n",
    "        basic_articles = self.scrape_google_news_articles(company_name, max_articles)\n",
    "        \n",
    "        if not basic_articles:\n",
    "            return {\n",
    "                'search_query': company_name,\n",
    "                'scraped_at': datetime.now().isoformat(),\n",
    "                'total_articles_found': 0,\n",
    "                'articles': []\n",
    "            }\n",
    "        \n",
    "        # Step 2: Extract detailed content for each article\n",
    "        comprehensive_articles = []\n",
    "        \n",
    "        for i, basic_article in enumerate(basic_articles):\n",
    "            print(f\"\\nProcessing article {i+1}/{len(basic_articles)}\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            # Merge basic data\n",
    "            comprehensive_article = {\n",
    "                'article_id': i + 1,\n",
    "                'google_news_data': basic_article,\n",
    "                'detailed_data': None,\n",
    "                'extraction_success': False\n",
    "            }\n",
    "            \n",
    "            if basic_article['google_news_url'] != 'URL not found':\n",
    "                # Get the actual article URL\n",
    "                try:\n",
    "                    actual_url = self.get_redirect_url(basic_article['google_news_url'])\n",
    "                    print(f\"Actual URL: {actual_url}\")\n",
    "                    \n",
    "                    # Extract detailed content (always extracts metadata and summary)\n",
    "                    detailed_data = self.extract_detailed_article_data(actual_url, extract_full_content)\n",
    "                    \n",
    "                    if detailed_data:\n",
    "                        comprehensive_article['detailed_data'] = detailed_data\n",
    "                        comprehensive_article['extraction_success'] = True\n",
    "                        print(\"âœ“ Successfully extracted detailed content\")\n",
    "                        if not extract_full_content:\n",
    "                            print(\"âœ“ Summary and metadata extracted (full content excluded)\")\n",
    "                    else:\n",
    "                        print(\"âœ— Failed to extract detailed content\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Error processing article: {str(e)}\")\n",
    "            \n",
    "            comprehensive_articles.append(comprehensive_article)\n",
    "            \n",
    "            # Add delay between requests\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Compile final result\n",
    "        result = {\n",
    "            'search_query': company_name,\n",
    "            'scraped_at': datetime.now().isoformat(),\n",
    "            'total_articles_found': len(comprehensive_articles),\n",
    "            'successful_extractions': sum(1 for article in comprehensive_articles if article['extraction_success']),\n",
    "            'full_content_included': extract_full_content,\n",
    "            'articles': comprehensive_articles\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def save_comprehensive_data(self, data, filename=None):\n",
    "        \"\"\"Save comprehensive data to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            content_type = \"full\" if data.get('full_content_included', True) else \"summary\"\n",
    "            filename = f\"comprehensive_news_{content_type}_{data['search_query'].replace(' ', '_')}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nComprehensive news data saved to: {filename}\")\n",
    "        return filename\n",
    "\n",
    "    def print_summary(self, data):\n",
    "        \"\"\"Print a summary of the scraped data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"COMPREHENSIVE NEWS SCRAPING SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Search Query: {data['search_query']}\")\n",
    "        print(f\"Scraped At: {data['scraped_at']}\")\n",
    "        print(f\"Total Articles Found: {data['total_articles_found']}\")\n",
    "        print(f\"Successful Detailed Extractions: {data['successful_extractions']}\")\n",
    "        print(f\"Full Content Included: {'Yes' if data.get('full_content_included', True) else 'No (Summary only)'}\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        for article in data['articles']:\n",
    "            print(f\"\\nArticle {article['article_id']}:\")\n",
    "            print(f\"  Title: {article['google_news_data']['article_title']}\")\n",
    "            print(f\"  Source: {article['google_news_data']['source_title']}\")\n",
    "            print(f\"  Date: {article['google_news_data']['date']}\")\n",
    "            print(f\"  Detailed Extraction: {'âœ“' if article['extraction_success'] else 'âœ—'}\")\n",
    "            \n",
    "            if article['detailed_data']:\n",
    "                if 'full_content' in article['detailed_data']:\n",
    "                    print(f\"  Full Content Length: {len(article['detailed_data']['full_content'])} characters\")\n",
    "                else:\n",
    "                    print(f\"  Full Content: Not included (summary-only mode)\")\n",
    "                \n",
    "                if article['detailed_data']['content_summary']:\n",
    "                    summary_preview = article['detailed_data']['content_summary'][:100]\n",
    "                    print(f\"  Summary: {summary_preview}...\")\n",
    "                \n",
    "                if article['detailed_data']['meta_description'] != \"Meta description not found\":\n",
    "                    meta_preview = article['detailed_data']['meta_description'][:80]\n",
    "                    print(f\"  Meta Description: {meta_preview}...\")\n",
    "        \n",
    "        print(\"=\"*100)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the comprehensive news scraper\"\"\"\n",
    "    print(\"Enhanced Comprehensive News Scraper\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get user input\n",
    "    company_name = input(\"Enter the company name to search for: \").strip()\n",
    "    \n",
    "    if not company_name:\n",
    "        print(\"Please enter a valid company name.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        max_articles = int(input(\"Enter max number of articles to process (default: 3): \").strip() or \"3\")\n",
    "    except ValueError:\n",
    "        max_articles = 3\n",
    "    \n",
    "    extract_full = input(\"Include full article content in JSON? (y/n, default: n): \").strip().lower()\n",
    "    extract_full_content = extract_full == 'y'\n",
    "    \n",
    "    if not extract_full_content:\n",
    "        print(\"Note: Will extract summary, meta description, and all metadata but exclude full content from JSON\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = ComprehensiveNewsScraper(headless=True)\n",
    "    \n",
    "    # Scrape comprehensive news data\n",
    "    print(f\"\\nStarting comprehensive scraping...\")\n",
    "    comprehensive_data = scraper.scrape_comprehensive_news(\n",
    "        company_name, \n",
    "        max_articles=max_articles,\n",
    "        extract_full_content=extract_full_content\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    scraper.print_summary(comprehensive_data)\n",
    "    \n",
    "    # Save to file\n",
    "    save_choice = input(\"\\nSave results to JSON file? (y/n): \").strip().lower()\n",
    "    if save_choice == 'y':\n",
    "        filename = input(\"Enter filename (press Enter for default): \").strip()\n",
    "        if not filename:\n",
    "            filename = None\n",
    "        \n",
    "        saved_file = scraper.save_comprehensive_data(comprehensive_data, filename)\n",
    "        print(f\"Data saved to: {saved_file}\")\n",
    "    \n",
    "    print(\"\\nScraping completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
